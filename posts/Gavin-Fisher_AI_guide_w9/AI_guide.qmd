---
title: "Guide: AI Models"
author: "Gavin D. Fisher"
date: "2023-07-11"
categories: ["Week Nine", "AI Models", "Python", "Guide", "Google Colab", "PyTorch"]
execute:
  echo: false
toc: TRUE
---

# Full Guide on Housing Team's AI

This will be a guide to a general idea of how AI works, which models we made, and the results we got.

## How AI Works

First an intro to the general idea of how image classification artificial intelligence models work. Image classification models use deep learning to analyze and classify images into different categories. These models typically use convolutional neural networks, or CNNs, which are designed to handle visual data. Computers cannot see images the same way as us, they must look over images in multiple ways to understand them. A CNN can looks at neighborhoods of pixels to look at edges, textures, and corners to find common patterns in a set of images.

![](max_pool.png)

CNNs consist of multiple different layers which make up the neural network. After constructing a network we are able to 'train' the model on images. Our team used supervised learning to train the models, this simply means that we had pre-sorted categories for the model to learn from. A common saying is in this area is 'garbage in, garbage out', meaning if you feed the model bad data it will preform poorly. Sorting images for training models is a long process but it is necessary so that we can have the best output possible.

After a model is trained with data we can use it to predict which class new images are part of. For example if we train a model on images that have a picture of a house and images that have images of no house the model will predict whether a new image it has not seen before to have a house or to have no house.

## Training Images

Training images are the most important factor of how well a model will preform. We cannot tune the model to be better preforming if the input images for training are poorly chosen. This is a huge issue the AI Housing team ran into this summer, there are plenty of images available online of good houses but much less of houses with damages which we are trying to evaluate. The following numbers are the ratios of images for the house present, vegetation, siding, gutter, and roof models.

430:100, 200:40:170, 210:150:100, 200:130, 260:150

Overall the ratios of images are not too bad but the total numbers are bad. For every model the smallest number is the 'negative' attribute whether that is a damaged gutter or damaged roof etc. Early models built during this project used about 400 images in each category just to test how models work and they predicted somewhat decent. Having less than 200 images to train on was very problematic because with only training on a couple hundred images there is no way a model can correctly predict thousands of new images from this data. This has greatly affected the model accuracy for every model we built. Future DSPG AI Housing team members need to sort many more images to increase the accuracy of models created before they can be considered reliable.

## Google Colab

[Google Colab](https://colab.research.google.com/){target="_blank"} is an online Jupyter Notebooks Google service. The first pilot models built for this project was done on Jupyter Labs following these videos([Video 1](https://www.youtube.com/watch?v=19LQRx78QVU&list=PLgNJO2hghbmiXg5d4X8DURJP9yv9pgjIu&index=1&ab_channel=NicholasRenotte){target="_blank"} [Video 2](https://www.youtube.com/watch?v=jztwpsIzEGc&list=PLgNJO2hghbmiXg5d4X8DURJP9yv9pgjIu&index=2&ab_channel=NicholasRenotte){target="_blank"}). Google Colab serves the same purpose but is more user friendly to students not as familiar with code, as there is less to set up. Colab pairs with Google Drive so all data being used must be saved there first then exported if it is needed somewhere else. 

## Tensorflow Models

The first models we built used the Tensorflow and Keras libraries. If you are interested in seeing the program we wrote [navigate here](https://github.com/DSPG-2023/Housing/tree/main/models_algorithm){target="_blank"} and all of the model folders have ipynb files which can be opened in Google Colab. But a short recap, we used 70% of the data for testing, 20% for validation, and 10% for testing. The training data trains the model, the validation data validates that the model is training properly and not overfitting, and the testing data is used to test how well the model preforms after being trained. Next we can look at a sample of the data to see the different buckets that we sorted the images into.

![](sample.png)

Now we must build the neural network, using a sequential model we used relu activation layers, max pooling layers, and a softmax dense layer.


![](neural_structure.png)

![](summary.png)
After building the network you can train based on the input data. This is the siding model we are walking through which has three classes of data input, good siding, siding with chipped paint, and poor or damaged siding. 

![](train.png)

After the model is trained we can gather the accuracy and loss of the model. Accuracy is how accurate the model was at predicting classification of the testing data set while loss is the measure of how different a models predictions were from the actual labels. We want the accuracy as high as possible and loss as low as possible. Below an ideal loss and accuracy graph is shown, then the one for the siding model, then what overfitting looks like.

![](proper_loss.png)

![](loss_acc.png)

![](overfitting.png)



## SHAP Models

## PyTorch and CAM Models

## Conclusions
