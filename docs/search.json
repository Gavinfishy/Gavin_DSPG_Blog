[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Gavin is from Bolingbrook Illinois, an upcoming Junior at Iowa State University with a major in Computer Science and a minor in Data Science. He is part of the Cardinal Space Mining Club as part of the controls team and soon to be treasurer of the club. This blog’s purpose is to track Gavin’s work done with DSPG as part of the AI housing team.\n\nLinks to other members of the DSPG AI Housing Team\nUndergrads: Angelina, Kailyn\nTeam leads: Morenike, Sadat\nDSPG Blog, DSPG Home Page"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gavin’s DSPG Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nWeek Nine Blog\n\n\n4 min\n\n\n\nWeek Nine\n\n\nAI Models\n\n\nPyTorch\n\n\nPython\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide: AI Models\n\n\n12 min\n\n\n\nWeek Nine\n\n\nAI Models\n\n\nPython\n\n\nGuide\n\n\nGoogle Colab\n\n\nPyTorch\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Eight Blog\n\n\n5 min\n\n\n\nWeek Eight\n\n\nExcel\n\n\nWeb Scraping\n\n\nAI Models\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Eight\n\n\n1 min\n\n\n\nWeek Eight\n\n\nHousing Team\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Seven Blog\n\n\n3 min\n\n\n\nWeek Seven\n\n\nAI Models\n\n\nPython\n\n\nWeekend\n\n\nGoogle Colab\n\n\nPyTorch\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Seven Blog\n\n\n2 min\n\n\n\nWeek Seven\n\n\nPython\n\n\nExcel\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGuide: How to Evaluate a City\n\n\n25 min\n\n\n\nWeek Seven\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\nR\n\n\nGuide\n\n\nWeb Scraping\n\n\nGoogle Colab\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Six Blog\n\n\n2 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nWeekend\n\n\nPyTorch\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Six Blog\n\n\n7 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Five Blog\n\n\n3 min\n\n\n\nWeek Five\n\n\nAI Models\n\n\nPython\n\n\nGoogle Colab\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Five\n\n\n4 min\n\n\n\nWeek Five\n\n\nHousing Team\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Four Blog\n\n\n4 min\n\n\n\nWeek Four\n\n\nWeb Scraping\n\n\nProject Plan\n\n\nExcel\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Three Blog\n\n\n2 min\n\n\n\nWeek Three\n\n\nAI Models\n\n\nProject Plan\n\n\nJupyter Notebook\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Two\n\n\n1 min\n\n\n\nWeek Two\n\n\nHousing Team\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Two Blog\n\n\n2 min\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek One Blog\n\n\n2 min\n\n\n\nWeek One\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html",
    "title": "Guide: AI Models",
    "section": "",
    "text": "This will be a guide to a general idea of how AI works, which models we made, and the results.\n\n\nFirst an intro to the general idea of how image classification artificial intelligence models work. Image classification models use deep learning to analyze and classify images into different categories. These models typically use convolutional neural networks, or CNNs, which are designed to handle visual data. Computers cannot see images the same way as us, they must look over images in multiple ways to understand them. A CNN can looks at neighborhoods of pixels to look at edges, textures, and corners to find common patterns in a set of images.\n\n\nCNNs consist of multiple different layers which make up the neural network. After constructing a network we are able to ‘train’ the model on images. Our team used supervised learning to train the models, this simply means that we had pre-sorted categories for the model to learn from. A common saying is in this area is ‘garbage in, garbage out’, meaning if you feed the model bad data it will preform poorly. Sorting images for training models is a long process but it is necessary so that we can have the best output possible.\n\n\n\nNeural Network\n\n\nAfter a model is trained with data we can use it to predict which class new images are part of. For example if we train a model on images that have a picture of a house and images that have images of no house the model will predict whether a new image it has not seen before to have a house or to have no house.\n\n\n\nTraining images are the most important factor of how well a model will perform. We cannot tune the model to be better preforming if the input images for training are poorly chosen. This is a huge issue the AI Housing team ran into this summer, there are plenty of images available online of good houses but much less of houses with damages which we are trying to evaluate. The following numbers are the ratios of images for the house present, vegetation, siding, gutter, and roof models.\n430:100, 200:40:170, 210:150:100, 200:130, 260:150\nOverall the ratios of images are not too bad but the total numbers are bad. For every model the smallest number is the ‘negative’ attribute whether that is a damaged gutter or damaged roof etc. Early models built during this project used about 400 images in each category just to test how models work and they predicted somewhat decent. Having less than 200 images to train on was very problematic because with only training on a couple hundred images there is no way a model can correctly predict thousands of new images from this data. This has greatly affected the model accuracy for every model we built. Future DSPG AI Housing team members need to sort many more images to increase the accuracy of models created before they can be considered reliable.\n\n\n\n\nGoogle Colab is an online Jupyter Notebooks Google service. The first pilot models built for this project was done on Jupyter Labs following these videos(Video 1 Video 2). Google Colab serves the same purpose but is more user friendly to students not as familiar with code, as there is less to set up. Colab pairs with Google Drive so all data being used must be saved there first then exported if it is needed somewhere else. Google Colab is where majority of our models were built.\n\n\n\nThe first models we built used the Tensorflow and Keras libraries. If you are interested in seeing the program we wrote navigate here and all of the model folders have ipynb files which can be opened in Google Colab. But a short recap, we used 70% of the data for training, 20% for validation, and 10% for testing. The training data trains the model, the validation data validates that the model is training properly and not overfitting, and the testing data is used to test how well the model preforms after being trained. Next we can look at a sample of the data to see the different buckets that we sorted the images into.\n\nNow we must build the neural network, using a sequential model we used relu activation layers, max pooling layers, and a softmax dense layer.\n\n\nAfter building the network you can train based on the input data. This is the siding model we are walking through which has three classes of data input, good siding, siding with chipped paint, and poor or damaged siding.\n\nAfter the model is trained we can gather the accuracy and loss of the model. Accuracy is how accurate the model was at predicting classification of the testing data set while loss is the measure of how different a models predictions were from the actual labels. We want the accuracy as high as possible and loss as low as possible. Below an ideal loss and accuracy graph is shown, then the one for the siding model, then what overfitting looks like.\n\n\n\nOn a range of zero to one accuracy should be as close to one as possible while loss should be as close to zero as possible. The first image shows how these graphs should look to know you have a decent model. The second image shows what our graphs look like from the siding model. There are multiple possible issues that could be causing our graphs to look that way but the most probable reason right now is insufficient training data. Greatly increasing the training data will not perfect the accuracy and loss but this will certainly take us a step in that direction.\nThe third image shows another problem to watch out for once our model increases in quality. Overfitting is a phenomenon that occurs when a model becomes exceptional in the training data but cannot predict new images well. This can be avoided by increasing the data size or reducing model complexity. This is not yet a problem for any of the models used during this project but is an important concept to keep in mind for the future. Finally the model can be exported to be used for predictions.\n\n\n\nBelow is an image of how our program works as a whole.\n\nThe AI Housing team built seven models for evaluating addresses (more were made this is discussed later). We made three models for image quality testing, the house present model which checks if a house is present in the image, the clear image model which checks to see if the house is obstructed, and the multiple house model which checks if multiple houses are visible.\nThe image quality models were made to eliminate bad images and select the best image to evaluate when given a few. Images with no houses or houses that can barely be seen due to obstruction should not be evaluated and if multiple images are available we want the image with only one house visible. After running through the first few models, if there are still multiple images remaining the program randomly selects one to evaluate.\nNext are the attribute models, which we currently have four, vegetation, siding, gutter, and roof. The vegetation model determines if there is no garden, a garden present, or overgrown weeds and bushes. The siding model determines if a house has good siding, chipped paint, or damaged siding such as panels missing or cracks. The gutter model determines if a house has good gutters or damaged gutters and the roof model determines if a house has a good roof or a damaged roof. Once all these attributes are predicted by the models we return them to a csv file joined on the address that was evaluated. We also keep track of the confidence percentage of each prediction which can be used for evaluation of the models confidence vs accuracy.\n\n\n\nSHapley Additive exPlanations or SHAP is a technique to explain predictions made by machine learning models. This roots from the Shapley value, which is a cooperative game theory that evaluates how important each player is for the overall cooperation and what payoff they can expect. This theory can be applied to machine learning models by looking at local accuracy, misingness, and consistency to determine what parts of input were most influential in the model making a prediction. Read more here.\nThe SHAP models were made in Google Colab using Tensorflow and keras just as the earlier models plus sklearn. As shown below the SHAP model is built with many more convolutional layers but as a result the loss is very low and the accuracy is very high. The following screenshots are from the house present SHAP model.\n\n\n\nNext we show the SHAP images. What is interesting about these graphs is that the first image shows that the model is looking at the outline of the house while the second model is looking exclusively at the trees behind the house. Our best explanation is that there is not enough training data for the model to realize that it is looking for the house not the trees being partially cut off. Either way it is interesting to see graphs of which pixels of the image most greatly influence a models prediction.\n\n\nWe have just simply gotten these models to run properly, they are not yet implemented into any other code. The SHAP models need to be compared to our first models to see if there are accuracy differences and if one network works better than the other. Additionally this can be implemented into our program to return a folder of images for each address evaluated so that we can get an idea of what type of images need to be added to the training data.\n\n\n\nClass Activation Mapping or CAM is another way of displaying why a model made a prediction. Similar to the SHAP technique, CAM highlights important regions in an image that play a role in the models prediction making what looks like a heatmap. Read more here.\nThe first model we built for CAM used keras and ResNet50. To read more on ResNet go here. Residual Networks or ResNet are Deep Learning networks that basically allow skipping of convolutional layers if they harm the models accuracy. The first model also used ImageNet which is a dataset with millions of images used for training and evaluation computer vision models. So this model was a test of what was possible with using a prebuilt model and a large dataset. We randomly decided to choose panda images as a test to see how the program performed.\n\nThis had us very excited to use our own model and dataset to try and get the same result as the image above. Quickly it was discovered that the upwards of fifty layers in resnet could not be easily replaced by our eight layer models. So we decided to try and learn PyTorch (an alternative library to Tensorflow).\nThe torch library has many different functions than Tensorflow and the neural networks are constructed a little differently. Thanks to courses on DataCamp we were able to figure out how to use PyTorch and get some good results.\nThe second model built was a house presence model. We put this code in a python file instead of a jupyter notebooks file like what is produced in Google Colab just to test out the difference. The process of using our own training data and producing CAM images was a surprisingly quick process with minor hiccups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove is the progression of the house present CAM model. At first the heat bubbles showed up in what seemed to be random locations but what was a mystery is that they were the same in all test images. It turned out that the program was overriding the variable keeping track of what the model was doing for the prediction rather than making copies of the information.\nAfter this was fixed, the model started to show certain areas that it was looking at to make it’s prediction. We were not satisfied with the images because they still seemed to be random. At this point we had 400 images of houses and only 40 images of no house present so we spent time to almost double the amount of images with no house present to improve the model. After rerunning the program more heat spots were visible and seemed a little less random.\nNote two things, first it seems this model likes to look at the trees for it’s prediction just as the SHAP model and after more data was added the left image was correctly predicted no house present.\nHowever we were still not satisfied this being the last week there needs to be more to show that this program works than random blotches. So we took the model and replaced the data with 200 images of cats and dogs to see if the model was really working properly.\n\n\nAbove you can see the first image looks at the nose and some of the fur. We doubled the training data which resulted in the second image which has more red around one of the eyes. Going back to the panda image earlier, the more accurate the model is the more red we will see around the object that it identifies. This last cat image is so exciting to see because unlike the house images that seem to be random blotches, most of the color is concentrated on the cat’s face.\n\n\n\nOverall messing around with AI is pretty interesting, the worst part is definitely sorting training images for your models, but once you have a training set it’s fun. To reiterate it is much easier to go look at the code for the AI models to see what is happening. We got the models to predict correctly about 80% of the time and wish next years housing team luck on improving these models further."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "This will be a full guide on how to recreate what AI housing team has done this summer using Ogden IA as an example.\n\n\nFirst we need to go to Beacon to try and collect a list of addresses. The following link will bring you to Boone County and you need to scroll in on Ogden (West/center). Beacon Map View\nWe found a tool called Instant Data Scraper that you need for this process. An alternative to this is badly needed as you will see soon, but this application worked for the time being when we had no prior web scraping experience yet. DSPG students for summer of 2024, I beg you to try and scrape Beacon with your own spider before using this tool to save yourself from the cleaning you have to do.\nBelow I display an image of the Beacon interface then an image zoomed in on Ogden.\n\n\nNavigate to the buttons towards the top of the screen and find the cursor over a box. This is the select tool that we need to use for selecting properties. There is a limit of how many addresses you can grab at once so below is what it looks like if you try to grab every property at once:\n\nInstead we need to click on this button and select the polygon version.\n\nTo avoid the problem of too many addresses I typically select four segments of the town and merge them after exporting to csv files. As you can see below I used the polygon tool to select all parcels north of 216th Street(the large road below my box). The polygon selector has multiple points you can set, just make sure each parcel you want is within the shaded area or on the line then double click to select all parcels.\n\nNotice on the right hand side how all of the parcels you selected show up. This is where we want to scrape the information from. Open the instant data scraper tool from your extensions button (top right). Below you can see how the chrome extension automatically found the information to the right which now has a red marker around it. I deleted all of the columns in this interface except for resultitem because from our experience this holds the parcel number, address, and owner.\n\n\nGo ahead and export to a csv. The file is automatically saved as beacon.csv so go ahead and rename this file to ogden_n_216_st.csv. This is a temporary name just so you can easily find your Ogden files and know that the first grab was north of this road that we chose. Repeat this process until you have grabbed all of the parcels you desire, you have to close the instant data scraper and open it again for each new grab. Some towns I could scrape in one go while some took four cuts to collect all the data.\nNext we are simply merging the csv files. Choose one of the files to be the main one, then go csv by csv to copy and paste into the main csv file. You could make a script to do this for you but honestly it doesn’t take that long, a trick is when you press ctrl + shift + down arrow you can select the entire column. ctrl + down arrow will bring you to the bottom of the csv column in your main file. Leave out the top row that has resultitem. Also all of the boxes have - or #NAME? don’t worry yet there is more in these boxes they are just on newlines.\nRename the main file to ogden_addresses because this in this file we will clean the data then create address links. First in B2 place this function =TRIM(CLEAN(SUBSTITUTE(A2,CHAR(160),” “))). This will reformat the input text to be on one line without hidden characters. Usually your able to double click the bottom right of the cell and it will auto fill all the way down but most of these functions refuse to auto fill so you just need to grab the corner and drag to fill in B to be easily accessible text. Next copy all of the cells in column B and paste as text into column C.\nCopy column C into column D. Select all of column D, navigate to the Data tab, then text to columns in data tools. Select delimited then press next, deselect tab then select other and enter - into the box then select next, then finish. This will separate the data by parcel number, address, and owner. I deleted the owner name due to it being unnecessary information.\n\nFinally I noticed address needs to be trimmed so in a new column you can use the function =TRIM(A2) then paste the result as values back into the address column.\n\n\n\nWhat are the Google API Links? Here is an example of the beginning of an address from Grundy Center:\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nthis is followed by &key=(API Key). Pasting the entire link brings you to an image provided by Google street view if it exists. You could also use latitude and longitude coordinates instead of address but the image you get is not guaranteed to be a front image of the house. Google however, has a built in program to get the front of a house if you enter the address if it can find one.\nWe will continue to use the file from the previous section. First you need a url_start column to store the first half of the url which is always the same (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=). So in columns G, H, I, I have the url in the previous sentence, City (Ogden), then State (IOWA). Next we need to concatenate the full address with =CONCAT(F2, “,”, H2, ” “, I2). F2 is the house address, H2 is city, and I2 is State which results in 119 W SYCAMORE ST, OGDEN IOWA for my first address. Copy this column into the next and paste as values. Do not forget to try double clicking the bottom right of the cell to auto fill before dragging.\nIn the next column use =SUBSTITUTE(K2, ” “,”+“) which will replace all the white space with + which is neccesary for the link. Again paste as values into the next column. Next =CONCAT(G2,M2) will combine what is my url_start column and my full_address_+ column to get the entire url needed to run through my Google API scraper.Finally we need to place the values of this last column I named url_full into the first column so it is easily accessible by the python script.\n\nNow you can grab one of the links from the leftmost column and check that the link works. All I did was copy that part of the link into my browser, added &key=(API Key) and this was my result:\n\n\n\n\nFor this section you need to obtain a Google API key to scrape images from the Google street view API. You also need to download R studio or an IDE that can run R code. If you have access to the DSPG Housing repository there is a folder named complete links which has a grab_images.R file. Below is the code for grabbing Google images for Ogden, keep in mind downloading images takes a very long time many of the files of 3k photos have taken me upwards of an hour to download. I also do not know how to control where the image file is uploaded. My current directory is my blog and the images are uploaded there, I assume there is a way to change the directory in R studio.\nSome errors I ran into while scraping: Google does not like addresses that are combined such as 123/456 Main St and it will cause an error. Some addresses you pull will start with a # and Google will not accept this. Lines that have the #NAME? error I delete the row. Some addresses I pulled from Ogden did not have an address but just an owner name which caused my program to throw an error. If the address is empty or does not start with a number I delete the row. I manually deleted just over 100 rows from the Ogden set which had missing addresses and was blank or filled in with owner name. This is a good example of why a personalized scraper should be made because while the chrome extension is convenient and fast it pulls back many issues with the data which we may get by scraping ourselves.\n# Ogden\nog_data &lt;- read.csv(“~/GitHub/Housing/complete links/ogden_urls.csv”)\nurls_start &lt;- og_data[, 1]\nurls_full &lt;- paste(urls_start, “&key=”, sep = ““)\nurls_full_api_key &lt;- paste(urls_full, api_key, sep = ““)\n# creates folder and downloads all images\ndir.create(“ogden_google_images_folder”)\nfor(i in seq_along(urls_full_api_key)) {\n___ file_path &lt;- file.path(“ogden_google_images_folder”, paste0(“G_OG_”, og_data[i,6], “_.png”))\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n___ print(file_path)\n___ print(i)\n}\nThe only changes to the above code is that you must change the path to your CSV file, where og_data[, num] the number must be changed to the column in the CSV with the full url if it is not in the first column already and the second instance changed to where the address column is, names of image export files if you use a different city, source or city within paste0 (convention is source_city_address_ where source is G Google, Z Zillow, Etc and city is OG Ogden, G Grundy Center, etc), and finally you must add an api_key variable with your own api key. In R Studio you can run just the variable name and it will save into your environment. Make sure to delete the API Key from your code before pushing it somewhere public such as GitHub.\nSome common errors with the images coming in are as follows: If Google does not have an image it will give you an image does not exist image but this should be easily identified by our AI model through training. Quick note if you retrain the models I show later in this guide only put 2-3 of these error images as to not throw off the model, having 50 or so will make the model think that image has a very high correlation. Next I have had an odd issue where the same image will be used for multiple addresses. This is quite weird because I do not know where the image is getting pulled from. Maybe there is a default image that is getting pulled somehow? But it is not just one image for Grundy Center which has about 3,500 images we pulled I saw anywhere from 10-20% of the images were duplicates with different addresses. There were probably 8-10 different images that were duplicated for different images so this is a big problem that doesn’t always happen but is a mystery to me why this happens. But hey 80-90% good images is a passing grade.\n\n\n\nThese are the two videos I used from Nicholas Renotte Video 1 Video 2. The first video shows how to set up Jupyter Labs while the second video explains how his model works. I copied his code but altered it a little for the binary models but quite a bit for the multi model classifications.\nTo be honest I think the model in the next section can handle binary scenarios but because I do not fully trust that theory so I am going to show the code for the binary models as it is a little different. When I made my first model I used Jupyter Lab which for an experienced programmer it wasn’t horrible to set up the environment but for my teams sake and future DSPG members we switched to Google Colab. Google Colab is an online resource that can be edited by multiple users just as other Google applications.\nAccess to entire projects are available through our AI Housing team GitHub but I will walk through the house_present.ipynb file. I highly recommend going to our Housing repository in models_algorithm to look at models rather than this guide (download and open in Google Colab) as I think it is much easier to read and follow. First off, ipynb is the extension for Jupyter Notebooks which is python code written in individual cells so that you can run cell by cell rather than the entire program. This is helpful for testing and minor fixes but I suppose these can be written in normal .py (python) files.\nMake sure tensorflow, opencv, and matplotlib are downloaded. You can download these libraries with pip install tensorflow. !pip list will display a list of downloaded libraries to check if these are properly downloaded. This next step is vital for all Colab codes:\nfrom google.colab import drive drive.mount(‘/content/drive’)\n%cd “/content/drive/MyDrive/Colab Notebooks/house_present_model”\nThe from section will connect Google drive to Colab. I will explain where images go in the Training the Model section below but keep in mind you will need to download your images to Google drive to train the model. The third line of code is the path to my house_present_model within Google drive so you will need to replace that path with your path to the folder which holds the Google Colab ipynb file and data images. You do not have to worry about this for now follow along with the rest of the code then skip to Sorting Images then Training the Model section to run the code.\nI have my imports scattered through the rest of the document but I will list them all now:\nimport tensorflow as tf\nimport os\nimport cv2\nimport imghdr\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.metrics import Precision, Recall, BinaryAccuracy\nfrom keras.models import load_model\nAs you can see these models are built with Tensorflow and Keras. Next we grab the images which will be in the data folder.\ndata_dir = os.getcwd()+‘/data’\nWe want to remove bad images from our image dataset. First go through and delete all images 10 KB and smaller, then delete any files that are not images.\nimage_exts = [‘jpeg’,‘jpg’, ‘png’]\nfor image_class in os.listdir(data_dir):\n___ for image in os.listdir(os.path.join(data_dir, image_class)):\n___ ___ image_path = os.path.join(data_dir, image_class, image)\n___ ___ try:\n___ ___ ___ img = cv2.imread(image_path)\n___ ___ ___ tip = imghdr.what(image_path)\n___ ___ ___ if tip not in image_exts:\n___ ___ ___ ___ print(‘Image not in ext list {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\n___ ___ ___ except Exception as e:\n___ ___ ___ ___ print(‘Issue with image {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\nAt this point most of the images we have will be fine to use for training the model. To see a sample of the images you can use plt.imshow(img) or plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) followed by plt.show() to see a properly colored version.\nNext we need to separate the data into 0 or 1. The house_present model for example, I have two folders, one with images with no house and one with house images. The model will predict a decimal between 0 and 1 so if the model predicts .79 it will belong to the second category of house images. Below will show a sample of what this looks like.\ndata = tf.keras.utils.image_dataset_from_directory(‘data’)\ndata_iterator = data.as_numpy_iterator()\nbatch = data_iterator.next()\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\nfor idx, img in enumerate(batch[0][:4]):\n___ ax[idx].imshow(img.astype(int))\n___ ax[idx].title.set_text(batch[1][idx])\n\ndata = data.map(lambda x,y: (x/255, y))\ndata.as_numpy_iterator().next()\nNext we separate the data into training, validation and testing which should be separated by 70%, 20%, 10%. len(data) will show how many images will be used in each train. For my last run I had 7 images.\ntrain_size = int(len(data).7) val_size = int(len(data).2)+1 test_size = int(len(data)*.1)+1\ntrain_size+val_size+test_size\nThe last line should be equal to len(data) if test or validation is 0 you need to add numbers to the end as I did above. There is a better solution in the following AI model section that I will replace this with if I remember. Next we build the layers of the Neural Network.\nmodel = Sequential()\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’, input_shape=(256,256,3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(32, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=‘relu’))\nmodel.add(Dense(1, activation=‘sigmoid’))\nmodel.compile(‘adam’, loss=tf.losses.BinaryCrossentropy(), metrics=[‘accuracy’])\nmodel.summary()\nUsing the model that was just created now it needs to be trained.\nlogdir=‘logs’\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val.cache().prefetch(buffer_size=AUTOTUNE)\nhist = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[tensorboard_callback])\nNext we graph the loss and accuracy of the model.\nfig = plt.figure()\nplt.plot(hist.history[‘loss’], color=‘teal’, label=‘loss’)\nplt.plot(hist.history[‘val_loss’], color=‘orange’, label=‘val_loss’)\nfig.suptitle(‘Loss’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\nfig = plt.figure()\nplt.plot(hist.history[‘accuracy’], color=‘teal’, label=‘accuracy’)\nplt.plot(hist.history[‘val_accuracy’], color=‘orange’, label=‘val_accuracy’)\nfig.suptitle(‘Accuracy’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\n\nThen we can evaluate the quality of the model\npre = Precision()\nre = Recall()\nacc = BinaryAccuracy()\nfor batch in test.as_numpy_iterator():\n__ X, y = batch\n__ yhat = model.predict(X)\n__ print(yhat)\n__ pre.update_state(y, yhat)\n__ re.update_state(y, yhat)\n__ acc.update_state(y, yhat)\nprint(pre.result(), re.result(), acc.result())\nprint(f’Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}’)\nThen we test the model to see that it works visually. The directory in the following code refers to an image that I chose to test on in the same directory as the ipynb file and the data folder.\nimg = cv2.imread(‘/content/drive/MyDrive/Colab Notebooks/house_present_model/data/Copy of G_G_108 E AVE_.png’)\nresize = tf.image.resize(img, (256,256))\nplt.imshow(resize.numpy().astype(int))\nplt.show()\nyhat = model.predict(np.expand_dims(resize/255, 0))\nyhat\nif yhat &gt; 0.5:\n__ print(f’No house present’)\nelse:\n__ print(f’House present’)\nFinally we can save the model. Rename the h5 file to whatever model you are making and it will save to a new folder named models.\nmodel.save(os.path.join(‘models’,‘house_present_classifier.h5’))\n\n\n\nThis section is a review of the model that classifies an image when there are three or more categories. This model can also handle only two categories and may work better than the previous model. This model was somewhat based on the prior binary model but was just adjusted to handle more labels. This is also done on Google Colab so we start with mounting to Google Drive.\nfrom google.colab import drive\ndrive.mount(‘/content/drive’)\nThen we load in the images the same way. Make sure the images you will be using is in Google Drive in whatever directory you insert below.\n%cd “/content/drive/MyDrive/Colab Notebooks/DSPG Models/vegetation_quality_model”\ndata_dir = os.getcwd()+‘/data’\nWe then set the batch size and image size\nbatch_size = 32\nimg_height = 180\nimg_width = 180\ndata = tf.keras.utils.image_dataset_from_directory(\n__ data_dir,\n__ image_size=(img_height, img_width),\n__ batch_size=batch_size)\nWe then divide the data as 70% testing, 20% validation, and 10% testing.\ntrain_size = int(len(data)*.7)\nval_size = int(len(data)*.2)\ntest_size = int(len(data)-train_size-val_size)\nNext we show some of the images in the data set\n\nThis code makes the training process go very quickly\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val.cache().prefetch(buffer_size=AUTOTUNE)\nThen we build the neural network.\nnum_classes = len(class_names)\nmodel = Sequential([\n__ layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n__ layers.Conv2D(16, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Conv2D(32, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Conv2D(64, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Flatten(),\n__ layers.Dense(128, activation=‘relu’),\n__ layers.Dense(num_classes, activation=‘softmax’)\n])\nmodel.compile(optimizer=‘adam’,\n__ loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n__ metrics=[‘accuracy’])\nThen we train the model.\nepochs=10\nwith tf.device(‘/device:GPU:0’):\n__ history = model.fit(\n____ train,\n____ validation_data=val,\n____ epochs=epochs)\nThen we print the Accuracy and Loss\nacc = history.history[‘accuracy’]\nval_acc = history.history[‘val_accuracy’]\nloss = history.history[‘loss’]\nval_loss = history.history[‘val_loss’]\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label=‘Training Accuracy’)\nplt.plot(epochs_range, val_acc, label=‘Validation Accuracy’)\nplt.legend(loc=‘lower right’)\nplt.title(‘Training and Validation Accuracy’)\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label=‘Training Loss’)\nplt.plot(epochs_range, val_loss, label=‘Validation Loss’)\nplt.legend(loc=‘upper right’)\nplt.title(‘Training and Validation Loss’)\nplt.show()\nThen we evaluate the quality of the model.\npre = Precision()\nre = Recall()\nacc = Accuracy()\nfor batch in test.as_numpy_iterator():\n__ X, y = batch\n__ yhat = model.predict(X)\n__ yhat = np.array(tf.math.argmax(yhat,axis=1))\n__ print(y)\n__ print(yhat)\n__ pre.update_state(y, yhat)\n__ re.update_state(y, yhat)\n__ acc.update_state(y, yhat)\nprint(f’Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}’)\nmodel.save(os.path.join(‘models’,‘vegetation_quality_classifier.h5’))\nAgain this is much easier to just go into the GitHub for housing and find these models. By the time of finishing this section I now have two more models that produce SHAP and CAM images which take multiple labels to go find towards the end of this project.\n\n\n\nThis step follows the Google scraping section directly. Sorting images is a very time consuming process and we have not found a quicker way than manually sorting image by image. I have tried to make this process as efficient as possible with the following method. First make a copy of the image folder so that you have the original images if you need to sort multiple ways. Next go to the image folder and make folders for each sorting method you have. Below is an example of what it looked like while I was sorting for siding. I had a good siding, chipped paint, and poor folder which would be used for training the model. I also had an excess good folder and a delete folder. The excess good folder took a bulk of the good houses because you do not want your data amounts to be too uneven. If you have too many duplicates or too many images in one folder this can cause inaccuracies in the model’s predictions. The delete folder made the sorting go quicker to just throw images into that folder and delete them later rather than deleting bad images as they come.\n\nMake a new folder named data and put all of the categories into that folder. The data folder needs to go into the same directory as the ipynb file in google drive. As shown above the data folder needs to be accessible by Google Colab so that it can determine amount of categories and sort accordingly.\n\n\n\nIf you have a ipynb file set up to create a model and you have sorted images into their respective folders, this step is easy. The following image should be similar to how your directory should look. After linking your Google Colab this left panel will show up with your folders, it is necessary for your data folder to be in the same directory as your ipynb file in Google Drive.\n\nOnce your sure the images are accessible for the model all you have to do is run all of the cells. There is an option to run all but I recommend running cell by cell to address errors as they appear. The final line will return the trained model into a new folder named models. You can download the h5 file (the trained model) from Google drive to your local machine to use them in the algorithm I wrote to evaluate images.\n\n\n\nMake sure you have an IDE that can run python code. I recommend Visual Studio as it can run many languages and has libraries to make python look nice but pycharm is another popular python IDE.\n\n\nBefore using the python scripts to run your models we need to sort the images from the original Ogden folder into a format that the function can understand. The way I planned the storage of images is as follows: there is a parent folder that holds all images, within the parent folder there are folders for each individual city, and within city folders are folders named by all of the addresses in those cities, and finally within the address folders are images for each address from different sources. Inside of the housing channel folder models_algorithm exists a file image_folder_sorter.py. Each city that I sorted has it’s own algorithm for sorting because they are in different folders going to different folders. I will create one for Ogden below.\nimg_loc = os.path.expanduser(“~/Documents/downloaded google images/ogden_google_images_folder”)\nparent_folder = os.path.expanduser(“~/Documents/parent_folder_holder”)\naddress_folders = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\nfiles = os.listdir(img_loc)\nfor img in files:\n__ address = img.split(“_“)[2].strip()\n__ new_address_folder = os.path.join(parent_folder, address_folders, address)\n__ os.makedirs(new_address_folder, exist_ok=True)\n__ source_path = os.path.join(img_loc, img)\n__ destination_path = os.path.join(new_address_folder, img)\n__ shutil.copyfile(source_path, destination_path)\nAs shown above this algorithm grabs the Ogden image folder then copies it to another folder in which it is sorted by address (which is given by name of each image). Make sure to rename the directories if you have different folder names.\n\n\nI hope the following graphic is helpful in understanding how the images are stored.\n\n\n\n\nIn this section I will explain how to use your AI model to evaluate addresses purely through using my python scripts.\n\n\nI have two files, city_evaluator.py and house_evaluator.py. city_evaluator reads in images from the method stated above, it navigates to a parent folder which is full of cities, you can navigate to one of these cities to evaluate the images present in it’s folders. For each address folder there may be multiple images of the same address but each of these folders are fed to house_evaluator.\nhouse_evaluator has three models at the beginning, house_present which identifies if there is a house present in an image, clear_image which identifies if there is a clear image of the house (whether it is obstructed or clear), and multiple_houses which identifies if there are multiple images in the picture. These image models were made with the intention of filtering out bad images. If no houses remain the program returns with everything as false. If two good images exist the program randomly selects one of the two.\nIf an address has an identified good image we will then run it against all our attribute models. Currently there is a vegetation model, siding model, gutter model, and roof model. All of these models need to be better trained to become more accurate but right now serve as good examples of how to build and use AI models for this project. The ipynb files are located in the same location as the h5 files which can be downloaded and opened in Google Colab.\nFinally the program returns multiple variables to city_evaluator including whether there is a clear image, name of the test failed if it did, name of image used, whether image was randomly selected, vegetation, siding, gutter, and roofing predictions along with percent confidence. I will discuss how city_evaluator writes this information to a CSV file in the final section.\n\n\n\nI’m going to list everything you can alter as we go through adding the model here in case anything needs adjusted. This is starting in the house_evaluator.py file. First this screenshot is of the image names and image variable holders being initialized. If you gather images from sources other than what is labeled below you will need to add a variable everyplace these show up. This shouldn’t be too difficult, I only count four locations that you would need to add an image holder variable and a name variable.\n\nYou can skip past all of the house image quality models to what is now about line 220 which is right after the random picker operation. Lets say your new model identifies window quality, if no image remains then you need to add your variable to the list of variables returned here with windows as None and window confidence as 0\n\nNext scroll past all of the next models (vegetation, siding, gutter, and roof) I left a spot for your window model if this isn’t a hypothetical situation :). Whether your model is binary or has multiple labels changes how you must implement it. For a binary model you can look at the house_present model and the multiple_house model as an example while for a multi option model you can look at any of the attribute models. Below are screenshots of house_present followed by the siding_model, house_present may be misleading but keep in mind it needed to check if an image was present and delete or add so your attribute model will be altered a little from this code and more similair to the start and end of the siding model.\n\n\nFinally add your models variables to the final return statement.\n\nNavigate to city_evaluator.py and update the three spots where it handles variables. This should be simple just copy and paste the lines and change the variable name. An example of the code snippets below: 1. clear_image = attributes.get(“clear_image”)\n\n\n\nif (‘clear_image’ in list(df.columns)):\n__ pass\nelse:\n__ df[‘clear_image’] = None\n\ndf.at[i,‘clear_image’] = clear_image\n\n\n\n\n\n\ncity_evaluator.py already does the work for you, you just need to add links. First double check that all the variables mentioned in the last section is updated or the new values will not be pulled and written. The folder paths you need to change is the path to the parent folder that contains all of the images (below I show Ogden’s links), then you need to make a csv to hold all of your information. I do not think there is a generic database csv file we have but all of the categories can be copied from an existing one. At minimum the csv file must have all of the addresses from the csv file used to create Google urls or nothing will print.\n\nThen update the following lines with your city info and place them with the others in city_evaluator. You can see I just uncomment whichever city I want to run.\nmain_folder = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\ndf = pd.read_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’))\ndf.to_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’), index = False)\nNow the fun part. If everything worked correctly all you have to do is run city_evaluator and wait for quite a while for all the images to be processed. The terminal will look like below for while it runs through the models.\n\nOgden was smaller so ran a little faster but here is the final result of the current models I have evaluating Ogden:\n\n\n\n\nI think this method goes pretty quick overall. Scraping addresses and cleaning goes quick once you have the functions to clean the data, AI models take a while to understand at first but after one or two the process becomes quicker, and implementing models and printing to CSV files take a while to code the first time but alterations are easy. The worst part of this process is sorting images for models. It can take hours and you may have to do it multiple times to increase the accuracy of models.\nSteps for the rest of this project not included in this guide:\nI will implement a roof model\nI will better train the current models\nBut the best part to go find once it’s done is AI heatmaps which are images that show where in an image a computer is most influenced to make a prediction."
  },
  {
    "objectID": "posts/Gavin-Fisher_week1/Week_One.html",
    "href": "posts/Gavin-Fisher_week1/Week_One.html",
    "title": "Week One Blog",
    "section": "",
    "text": "Datacamp\n\nNine Lessons Completed: Intro to Deep Learning with Keras, Web Scraping in Python, Data Communication Concepts, Image Processing in Python, GitHub Concepts, AI Fundamentals, Introduction to ChatGPT, Introduction to R, Intermediate R. Below are a few examples of what I thought was cool in these lessons. Obviously too much content was covered to display here.\n\nIntro to Deep Learning with Keras\n\n# Import the Sequential model and Dense layer\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an input layer and a hidden layer with 10 neurons\nmodel.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n\n# Add a 1-neuron output layer\nmodel.add(Dense(1))\n\n# Summarise your model\nmodel.summary()\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n dense_2 (Dense)             (None, 10)                30        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 11        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 41\n\n\nTrainable params: 41\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\n\nNeural Network\n\n\n\n\nImage Proccessing in Python\n\n\n\nRocket Image\n\n\n# Import the modules from skimage\nfrom skimage import data, color\n\n# Load the rocket image\nrocket = data.rocket()\n\n# Convert the image to grayscale\ngray_scaled_rocket = color.rgb2gray(rocket)\n\n# Show the original image\nshow_image(rocket, 'Original RGB image')\n\n# Show the grayscale image\nshow_image(gray_scaled_rocket, 'Grayscale image')\n\n\n\nBlack and White Rocket\n\n\n\n\n\nGrapefruit\n\n\n# Import the canny edge detector \nfrom skimage.feature import canny\n\n# Convert image to grayscale\ngrapefruit = color.rgb2gray(grapefruit)\n\n# Apply canny edge detector\ncanny_edges = canny(grapefruit)\n\n# Show resulting image\nshow_image(canny_edges, \"Edges with Canny\")\n\n\n\nGrapefruit Edges\n\n\n\n\n\nBuilding\n\n\n# Import the corner detector related functions and module\nfrom skimage.feature import corner_harris, corner_peaks\n\n# Convert image from RGB-3 to grayscale\nbuilding_image_gray = color.rgb2gray(building_image)\n\n# Apply the detector  to measure the possible corners\nmeasure_image = corner_harris(building_image_gray)\n\n# Find the peaks of the corners using the Harris detector\ncoords = corner_peaks(corner_harris(building_image_gray), min_distance=20, threshold_rel=0.02)\n\n# Show original and resulting image with corners detected\nshow_image(building_image, \"Original\")\nshow_image_with_corners(building_image, coords)\n\n\n\nBuilding Corners\n\n\nI left out a lot of the cool images I got to make but I would definitely recommend Datacamp for people trying to learn data related topics with R, Python, and machine learning because I found these lessons very helpful and fun. I’m not sure how much it costs because it is provided through DSPG but I think it would be worth it to buy for a month or something and do as many lessons as possible during the summer or so."
  },
  {
    "objectID": "posts/Gavin-Fisher_week2/Week_Two.html",
    "href": "posts/Gavin-Fisher_week2/Week_Two.html",
    "title": "Week Two Blog",
    "section": "",
    "text": "Tidycensus and this Blog\nWe watched videos learning about the basics of Tidycensus from the developer of the package. It taught basic map creation and general use of the application link to video I made one of the graphics from the video and one from his textbook online (I excluded the images made with the walk through). This blog was also created during the second week. Otherwise we started to look into our project.\nlibrary(tigris)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\nia_pumas &lt;- pumas(state = \"IA\", cb = TRUE, year = 2019)\nia_puma_map &lt;- mapview(ia_pumas)\n\n\n\nPumas in Iowa\n\n\nDM_IA_tracts &lt;- map_dfr(c(\"IA\"), ~{\n  tracts(.x, cb = TRUE, year = 2021)\n}) %&gt;%\n  st_transform(8528)  \n\nDM_metro &lt;- core_based_statistical_areas(cb = TRUE, year = 2021) %&gt;%\n  filter(str_detect(NAME, \"Des Moines\")) %&gt;%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = DM_IA_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = DM_metro, fill = NA, color = \"red\") + \n  theme_void()\n\n\n\nOutlined Des Moines metro\n\n\nThere are many more cool graphs to make with this library that I did not touch. Between Kyle Walker’s free book online and his lectures on YouTube there is tons to learn and do with the census data using this library. Hopefully this will be helpful later in our project and I will get to learn more."
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using Iowaassessors.com I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. I was responsible for the new vegetation model and siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example, for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels. The new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\nWe also went to the ITAG conference. I went to Bullwall by Don McCraw, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule. I found the Bullwall talk very interesting which explained generally how cybersecurity works and how their product is an extra protection layer."
  },
  {
    "objectID": "posts/Gavin-Fisher_week6/Week_Six.html",
    "href": "posts/Gavin-Fisher_week6/Week_Six.html",
    "title": "Week Six Blog",
    "section": "",
    "text": "Monday DSPG assessed Grundy Center and New Hampton by taking pictures of houses and tracking characteristics of the houses. Tuesday we had a similar experience assessing Independence. A few details came to mind during these events, first in the Fulcrum app we had the option to evaluate roofs labeled AI Roof.\nAssuming that in future years roofing will be added to the list of characteristics evaluated I thought that when looking at roofs you need not only assess the quality of the roof but also the age. I believe there is a difference between a faded old roof and a new damaged roof. If DSPG goes to cities assessing the houses again it would be wise to sit down as a group and discuss what is considered good, fair, and poor in every single category. While assessing with other DSPG members I realized that people’s opinions of these categories greatly differed.\nWhile talking to residents of these towns I was made aware of other issues besides what we were evaluating such as plumbing and flooding issues. Although these are different than what the Housing team is currently directed towards maybe it could be projects for the future in DSPG.\nDue to the AI Roof category I left a space for the roofing model. If I have extra time I will fill this space with two models, one that evaluates whether the roof is damaged and one that evaluates the age. Additionally I added a spot for an AI window evaluator for a model that can predict whether there is a boarded up or broken window, inspired by Google images I downloaded from addresses considered poor or very poor in Des Moines. An option to look for in future years is an AI model that can look for multiple characteristics such as age and damage for roofs. Another we discussed this week is AI heatmaps which show where the AI is looking in the image to determine it’s guess. I really want to attempt this time permitting.\nNow for reading relief:\nI made this graph real quick just to visualize the counties which we are focusing on. Grundy Center is in Grundy County, New Hampton is in Chickasaw County, Independence is in Buchanan County, and Slater is in Story County.\n\nHere is a closer look at the fulcrum app we used to evaluate houses. The first image shows the different folders for each community, followed by the interface for a singular house evaluation.\n\n\n\nNext for the code I built on the skeleton from last week mostly following the code plan diagram I made 2 weeks ago(?). As of right now house_evaluator.py can read in a folder with images in it (as if it was one house from multiple sources). The program will use the house_present_classifier, clear_image_classifier, and multiple_houses_classifier models. These models evaluate whether each image has a visible house, whether the house is obstructed, and if there are multiple houses visible. Ideally we want to evaluate an image with a house, minimal obstruction, and only one house visible. Each model will remove unwanted images from the list in the hopes of choosing only the best image of the house to evaluate.\nIf no image remains the program returns that a better image is needed of the house. If multiple good pictures of the house remain the program will randomly select one. I hope to implement looking at the date of the images before randomly selecting but we do not have dates of images stored from any source yet.\n\nNext the remaining image will be ran through the vegetation, siding, and gutter models. As I said last week the gutter model is on hold until I get more images of poor gutters and spaces in the code are there for roof and window models for either next year or my free time. All of the models predict pretty poorly still because we need a lot more data on bad house images but they are predicting at a 35-45% confidence rate currently.\nAfter I got all of the models in the program properly I started to work on writing to CSV files. The intention was to print each attribute such as vegetation in it’s respective column based on address. I used the CSV library which turned out to take longer than it seems giving me issues such as wiping the file clean, not writing any values, then writing values to the leftmost row available after address. I finally got the attributes to print to the correct column but printed on every row instead of only one row based on address.\nSadat, our AI fellow, used Pandas and could correctly do the correct task in about 10 lines where it took my method about 40 lines of code. Long story short I am now implementing pandas to manipulate the csv files.\nThis code shows the program checking if the columns exist in the csv and if not adding the column. Then the values of each is wrote to the CSV file. After discussion with Sadat I added the model confidence percentage so that as we upload more accurate models we can see how well they are preforming and also give an average confidence for a final report.\n\n\nIn summary this program so far can take in a folder of images, choose the best one, evaluate it’s vegetation, siding, and gutters, then print these attributes to a csv at the same address as the input folder. This python Script was nearing 400 lines of code due to tests and old code I commented out to save. To run many images through this program I think the easiest path would to make a parent folder full of folders labeled by address and inside of the address folders are the images for each address. I did not want a 400 line for loop to iterate through this parent folder so my goal for this weekend is to make two scripts.\nThe first script will read in the files and iterate through them calling the second script each run to evaluate the images within and return house attributes. The first script will take these attributes and return them to a CSV file to the proper column and address. This will be essentially the same as what I have working now but it will be able to read and document many folders of houses rather than individual houses manually placed at a time. I will leave the current script alone since it works and make two completely new scripts to improve this method.\n\n\nToday was very productive, the entire program now works. city_evaluator.py can read in a parent folder which contains address folders, it then calls house_evaluator.py for each address folder. house_evaluator runs images through the AI models (still largely inaccurate) then returns attributes of the image detected. Having multiple files was accomplished by creating a function within house_evaluator then calling that function in city_evaluator.\n\n\nAfter loading in all the variables in city_eval I wrote to the csv by address. This method largely stayed the same from yesterday I just had to tweak it so that the name being printed to was iterated.\n\n\nIn my actual conclusion I will say this was a very successful week even though it was shortened. I plan to make a blog post just for my weekend project so that I don’t have to remember it next Thursday. I will attempt to make an AI heatmap so we can visually see why the models guess incorrectly (right now they suck because we barely have any data). A lot of people I have talked to think of object detection when I say AI heatmap so I will need to explain in depth why this is different. Also in the image above you can see gutter is FALSE and 0 confidence because I have still neglected making that model. I now have access to data maybe I can get it made next week. We also REALLY need to update the other models with the data we collected in Grundy Center, Independence, and New Hampton."
  },
  {
    "objectID": "posts/Gavin-Fisher_week7/Week_Seven.html",
    "href": "posts/Gavin-Fisher_week7/Week_Seven.html",
    "title": "Week Seven Blog",
    "section": "",
    "text": "First I had to resort Google images into folders that could be processed by my city_evaluator.py file. I made a diagram to show how the images are being stored below.\n\nI made an image sorting algorithm that takes a source such as Google’s images then sort each image based on address and make a folder for each address. The reason each town folder has folders with address names is so that when I pull in winvest images as I do below I can have multiple images for the same address to run through the AI models. The next images just show the parent folder and how the folders are stored same as the above graphic.\n\n\nAn update from last week was that instead of printing to a test file I am now able to update each CSV with information needed. I added a test_failed category to show which image quality model kicked the address back and I added the title of the image used for the evaluation. Also added is the gutter model and a space for the roofing model. The first image below shows what happens when no image is found for an address. The second is a picture of New Hampton being evaluated. The reason it is missing multiple values is because the Google streetview had a lot of missing images for New Hampton. I added a third image which is Independence which is filled out from the later part of the week so it is full and has more attributes.\n\n\n\nI made a full guide of how to recreate the results of this project code wise here. I plan to do more before the summer finishes but this guide covers how to scrape images, clean data, make urls, scrape the Google API for streetview images, make models, implement models, and finally export results of models to a CSV for each address. Our team also made the teaser video for our AI housing project.\nI have also made an algorithm that will rename photos from Winvest so that I can sort it into the parent folder and have more than just Google images to evaluate. A lot of this week has been updating and improving the code from last week so that everything works better. On my todo list includes adding the roof model, retrain existing models to improve accuracy, and try to make heatmaps by the end of the project."
  },
  {
    "objectID": "posts/Gavin-Fisher_week8/Week_Eight.html",
    "href": "posts/Gavin-Fisher_week8/Week_Eight.html",
    "title": "Week Eight Blog",
    "section": "",
    "text": "First I took a sample of the data bases to see how accurate the models were predicting. I used about 25 addresses from each source (Grundy Center, Independence, New Hampton, Slater, and Ogden) which totaled to 118 addresses after removing the empty evaluations from New Hampton. This is a very small sample considering we have a couple thousand addresses in our database. I went one by one checking whether each attribute was predicted properly for each image.\nEach model returns it’s confidence of each prediction it makes so I took the average as follows, vegetation: 43.2%, siding: 37.02%, gutter: 65.07%, roof: 54.89%. Again, this shows the confidence of how sure the model is of it’s prediction so for example on average the gutter model is 65% sure on average that it’s prediction is correct.\nNext I assigned a 0 for false or 1 for true for each model’s prediction on whether it’s guess was accurate compared to the image I looked at. This gave me a percent of accurate guesses in the sample data as follows, house present model: 84.62%, vegetation model: 86.42%, siding: 3.7%, gutter: 82.72%, roof: 88.89%. You may have noticed that the siding model only had a 3.7% accuracy rate which is what caused this sample to be evaluated. I believe that the siding model’s real percentage is 96.3% guessed correctly but that the program to run the models returns the wrong prediction from the model. This should be a quick fix then all of the databases need to be updated. Back to the rest of the models, the percentages are pretty high but I do not think that this is due to the accuracy of the models. I think the models are predicting that the attributes are in good condition most of the time and it just so happens that most houses do not have issues. For example the gutter model had 82.72% accuracy from the sample but many of the images gutters were barely visible which leads me to believe that the model is looking at other house characteristics than actually looking for gutters.\nNext some issues I found with this sample:\n\nThe roofing model predicted bad roof when no roof is visible. This may still be desirable because gutter does the same thing where is predicts bad gutter when none is visible in the image\nA small blur spot in the top of the image caused the image to be marked as no house present\nRand select chose an image of a house from a street corner (Google) rather than the front of a house (Winvest) which means the house present model should have removed the street corner image\nVery visible house marked no house present\nCornfield marked as house present\nDuplicated addresses\nImage of the inside of a store marked house present\n\nI wrote comments of every issue in one of the sample data csv columns.\n\n\nNext I made a list of test cases to test the models and overall program. Our team will not be able to finish creating these tests but it is important for next year so when they improve the models they have a basic testing plan.\n\n\n\n\nAaron Case on the local foods team was kind enough to spend some time creating a spider to scrape Beacon for information on houses in Independence. I am not sure if we will be providing next years AI Housing team with this code (depends if he shares or not, if not spiders aren’t horrible just take a lot of time to make).\nAaron’s spider scraped image urls and some other information on the Independence homes but the most helpful part was the image links. This provides us with three sources for some addresses in Independence to evaluate. I made a python script to read in these urls and grab all of the images to use for evaluation. After creating this program sorting the images and implementing them into the Independence database was pretty straightforward.\n\n\n\nAfter some tweaking of the model Sadat helped me get the SHAP images to show up and they are quite interesting to see.\n\nThe red spots in the bottom graphic show which pixels in the house image are most influential in determining that a house is present. As you can see the trees are highlighted so the model is determining that a house is present because it cannot see the full shape of the tree.\n\nThis second image shows what we want the model to see more. It is still looking at the trees but it is also looking at the house to make it’s prediction. This is largely due to the lack of training data we have provided the models to train on because most images we have given the model have trees in the image.\nAlthough it is disappointing to see that the model is looking at the trees more than the houses this is an important step in understanding how the models learn from the images we provide."
  },
  {
    "objectID": "posts/Gavin-Fisher_week9/Week_Nine.html",
    "href": "posts/Gavin-Fisher_week9/Week_Nine.html",
    "title": "Week Nine Blog",
    "section": "",
    "text": "This is our final week with our final presentation on Friday.\nFirst I made a guide on AI used during this project here. This guide is for the Friday presentation but also future DSPG students.\n\n\nI am so glad to announce that I got the CAM model working as of Monday morning. It turns out that in the weekend Seven blog I almost had the entire model running properly. What I found during this eighth weekend is that instead of tracking the weights as the model predicted I was rewriting the value. So it was a simple fix of adding .clone() to make clones of the x variable. Last week it seemed that the heat map marks were showing up randomly and was the same in every test image but after my tweak they seemed random still but now different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference between the second and third row of images is the amount of data. As I increased the amount of data the accuracy of the model increased with it so the red spots increased in size. I realized that the model may preform better if I used it on different data that was more simple such as cats and dogs. I think the reason the house present model is struggling in prediction is due to the trees in the background but as data increases accuracy should too.\nI duplicated the model to train on cats and dogs instead to see how well it worked when more simple data was available. The house present model had a ratio of about 400 images of houses present and 40 images of no house present which is a bad ratio while dog and cat images from Google I started with 100 of each category then increased to 200 in each category.\n\n\nI was very excited to see these images because it showed that the CAM model was working properly. The house present category is difficult to determine due to trees and we allow houses to be in the background if it is a picture of an empty lot but it preforms much better on images of cats and dogs. With just 200 images of cats the model was able to look at the eyes and nose to determine that it was a cat. The second image has more coloring because it had more training data. An image that would show very high accuracy and confidence can be shown with the model I used to evaluate pandas during the 6th weekend here.\n\nNext I downloaded all of the images we have so far into box to replace the messy images that were there. I have made programs to download new Google Images and sort them so if I deleted extra images next year can download new ones. I will delete old Google images and upload our training sorted images by the end of the week.\nSome time was spent writing recommendations for next year. I wrote most of the code and model recommendations, I hope these blogs will be helpful to get the next team started on the project.\nHuge issue I just found, the h5 file produced for the SHAP model is too big to upload to GitHub. The maximum is 100 MB and the h5 file is 197 MB. I decided that this is not my problem (sorry next year) but I uploaded the code to construct new models. There is a process to upload large files through git here but I am not sure if other people in the project also need to do this so I just left the model out.\nThis is the final information I will ever put in this blog :). Today is the last day of work all we did was look over the presentation and edit some things. Below I’ll post the updated graphics I made. Adios hope you enjoyed reading through this blog."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "href": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "title": "Weekend Six Blog",
    "section": "",
    "text": "This weekend I set out to make what I refer to as AI heatmaps which is created when an image classification model predicts a class we also track what pixels in the image helped influence it’s prediction. I tried to use pytorch but after lots of code and lots of error I put a temporary pause and explored other libraries. I ended up using keras and resnet for my class activation mapping python script by following along with a video.\n\nAlthough I think this looks like a heatmap I think that the proper name is a class activation map or CAM. Maybe I will refer to it as AI heatmaps anyway.\nFrom this video I got a majority of the code but some of the libraries were outdated so I had to use the pillow library for images rather than keras images. Resnet also seemed to change the way you could call layers so I had to change the call to the exact name of the layer which could be really inefficient if the amount of layers change. Otherwise the code as of right now is pretty much the same.\n\n\nI want to alter this code a little however. The classifiers and models are not the ones we built, so I think it would be pretty cool if we could implement our housing models into this method to see how well the models are operating with visual examples. Knowing that our models are not very accurate this could help us to know what specific features we need more images of or which features give the model false guesses."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html",
    "href": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html",
    "title": "Weekend Seven Blog",
    "section": "",
    "text": "During this long weekend I learned more about AI models and toyed around with the heat maps I wanted to get done. The SHAP method mostly works and the CAM method prints but does not work quite yet and I will show both of these below.\nFirst I finished a course in DataCamp labeled Deep Learning with PyTorch. This course gave me a lot more insight on how to build layers of Neural Networks and gave some cool graphics I wanted to display after my ramble.\nI learned about some of the math behind machine learning such as matrix multiplication, derivatives, and how weights are set to each connection of the network.\n\n\n\n\nNext I made an AI model that prints SHAP images from it’s prediction. I used the gutter model first then made another for house present. Currently the image is fed to the AI too blurry for it to see the gutter so I need to go back and give the AI a more detailed image to evaluate but in the meantime I made another model for the house present data.\nThese models use keras and sklearn to build the network just as our previous models and then uses the shap library to create images that show basically what the AI is using to make it’s prediction. These models were also made in Google Colab.\n\n\n\n\nAs you can see I tried to display the data a few different way’s and they definitely need to be tweaked. The last three images are the SHAP images, my favorite is the last one. It is showing all of those empty graphs for some reason but the actual images shown is what we want. It shows which pixels the model is looking at to make each determination. You may notice the two images are basically the same but this makes sense because if the model determines one line of pixels shows a house is probably present the model will use the same pixels to determine that the no house present category is false.\nFinally I tried to apply what I learned about PyTorch to make a CAM model. I made this model in a Py file just to see the difference of running in Google Colab. This model uses torch, torchvision, and sklearn.\n\n\nI got the image to display but as you can see for an example of a house present and no house present the heat map is exactly the same which probably means it is not tracking the neural network properly.\nI think this all is a good step forward and hopefully was enough to finish this by our presentation next week.\nAlso it’s Independence Day, God Bless the USA"
  },
  {
    "objectID": "posts/House_blog/House_blog.html",
    "href": "posts/House_blog/House_blog.html",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#happies",
    "href": "posts/House_blog/House_blog.html#happies",
    "title": "Housing Team Recap Week Two",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#crappies",
    "href": "posts/House_blog/House_blog.html#crappies",
    "title": "Housing Team Recap Week Two",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "href": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Recap Week Two",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "href": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "title": "Housing Team Recap Week Two",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\nKailyn\n\n\n\n\n\nGavin\n\n\n\n\n\nKailyn\n\n\n\n\n\nAngelina\n\n\n\n\n\nKailyn\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "href": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "title": "Housing Team Recap Week Two",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "posts/House_blog/House_blog.html#questions-discussion",
    "href": "posts/House_blog/House_blog.html#questions-discussion",
    "title": "Housing Team Recap Week Two",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "posts/Housing-Week-Eight/House_Week_Eight.html",
    "href": "posts/Housing-Week-Eight/House_Week_Eight.html",
    "title": "Housing Team Recap Week Eight",
    "section": "",
    "text": "Shortened blog this week.\n\n\n\nFinished editing demographic profile plots\nState, regional, and national context added to graphs\nAdded percentages to some of the graphs\n\n\n\n\n\n\nCreated house attribute quality dashboards\nMade a guide on how to create the maps\nPlan to make graphs on AI model accuracy vs actual house quality\n\n\n\n\n\n\nThanks to Aaron’s spider, Beacon images were added to the Independence database evaluation\nSHAP model works on our own datasets (Sadat fixed the code)\nPlan to have SHAP models for each attribute to visualize how well the models are preforming\n\n\n\n\n\n\n\nFinish demographic analysis for Iowa communities\nGraph AI confidence vs accuracy\nImplement SHAP models into program (and hopefully CAM too!)"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html",
    "href": "posts/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html#week-five-ai-housing-team",
    "href": "posts/Housing-Week-Five/House_Week_Five.html#week-five-ai-housing-team",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#project-milestones",
    "href": "posts/House_blog/House_blog.html#project-milestones",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/Gavin-Fisher_week8/Week_Eight.html#evaluation-sample-test-cases-beacon-images-shap",
    "href": "posts/Gavin-Fisher_week8/Week_Eight.html#evaluation-sample-test-cases-beacon-images-shap",
    "title": "Week Eight Blog",
    "section": "",
    "text": "First I took a sample of the data bases to see how accurate the models were predicting. I used about 25 addresses from each source (Grundy Center, Independence, New Hampton, Slater, and Ogden) which totaled to 118 addresses after removing the empty evaluations from New Hampton. This is a very small sample considering we have a couple thousand addresses in our database. I went one by one checking whether each attribute was predicted properly for each image.\nEach model returns it’s confidence of each prediction it makes so I took the average as follows, vegetation: 43.2%, siding: 37.02%, gutter: 65.07%, roof: 54.89%. Again, this shows the confidence of how sure the model is of it’s prediction so for example on average the gutter model is 65% sure on average that it’s prediction is correct.\nNext I assigned a 0 for false or 1 for true for each model’s prediction on whether it’s guess was accurate compared to the image I looked at. This gave me a percent of accurate guesses in the sample data as follows, house present model: 84.62%, vegetation model: 86.42%, siding: 3.7%, gutter: 82.72%, roof: 88.89%. You may have noticed that the siding model only had a 3.7% accuracy rate which is what caused this sample to be evaluated. I believe that the siding model’s real percentage is 96.3% guessed correctly but that the program to run the models returns the wrong prediction from the model. This should be a quick fix then all of the databases need to be updated. Back to the rest of the models, the percentages are pretty high but I do not think that this is due to the accuracy of the models. I think the models are predicting that the attributes are in good condition most of the time and it just so happens that most houses do not have issues. For example the gutter model had 82.72% accuracy from the sample but many of the images gutters were barely visible which leads me to believe that the model is looking at other house characteristics than actually looking for gutters.\nNext some issues I found with this sample:\n\nThe roofing model predicted bad roof when no roof is visible. This may still be desirable because gutter does the same thing where is predicts bad gutter when none is visible in the image\nA small blur spot in the top of the image caused the image to be marked as no house present\nRand select chose an image of a house from a street corner (Google) rather than the front of a house (Winvest) which means the house present model should have removed the street corner image\nVery visible house marked no house present\nCornfield marked as house present\nDuplicated addresses\nImage of the inside of a store marked house present\n\nI wrote comments of every issue in one of the sample data csv columns.\n\n\nNext I made a list of test cases to test the models and overall program. Our team will not be able to finish creating these tests but it is important for next year so when they improve the models they have a basic testing plan.\n\n\n\n\nAaron Case on the local foods team was kind enough to spend some time creating a spider to scrape Beacon for information on houses in Independence. I am not sure if we will be providing next years AI Housing team with this code (depends if he shares or not, if not spiders aren’t horrible just take a lot of time to make).\nAaron’s spider scraped image urls and some other information on the Independence homes but the most helpful part was the image links. This provides us with three sources for some addresses in Independence to evaluate. I made a python script to read in these urls and grab all of the images to use for evaluation. After creating this program sorting the images and implementing them into the Independence database was pretty straightforward.\n\n\n\nAfter some tweaking of the model Sadat helped me get the SHAP images to show up and they are quite interesting to see.\n\nThe red spots in the bottom graphic show which pixels in the house image are most influential in determining that a house is present. As you can see the trees are highlighted so the model is determining that a house is present because it cannot see the full shape of the tree.\n\nThis second image shows what we want the model to see more. It is still looking at the trees but it is also looking at the house to make it’s prediction. This is largely due to the lack of training data we have provided the models to train on because most images we have given the model have trees in the image.\nAlthough it is disappointing to see that the model is looking at the trees more than the houses this is an important step in understanding how the models learn from the images we provide."
  },
  {
    "objectID": "posts/Gavin-Fisher_week6/Week_Six.html#data-collection-and-python-evaluator-improvements",
    "href": "posts/Gavin-Fisher_week6/Week_Six.html#data-collection-and-python-evaluator-improvements",
    "title": "Week Six Blog",
    "section": "",
    "text": "Monday DSPG assessed Grundy Center and New Hampton by taking pictures of houses and tracking characteristics of the houses. Tuesday we had a similar experience assessing Independence. A few details came to mind during these events, first in the Fulcrum app we had the option to evaluate roofs labeled AI Roof.\nAssuming that in future years roofing will be added to the list of characteristics evaluated I thought that when looking at roofs you need not only assess the quality of the roof but also the age. I believe there is a difference between a faded old roof and a new damaged roof. If DSPG goes to cities assessing the houses again it would be wise to sit down as a group and discuss what is considered good, fair, and poor in every single category. While assessing with other DSPG members I realized that people’s opinions of these categories greatly differed.\nWhile talking to residents of these towns I was made aware of other issues besides what we were evaluating such as plumbing and flooding issues. Although these are different than what the Housing team is currently directed towards maybe it could be projects for the future in DSPG.\nDue to the AI Roof category I left a space for the roofing model. If I have extra time I will fill this space with two models, one that evaluates whether the roof is damaged and one that evaluates the age. Additionally I added a spot for an AI window evaluator for a model that can predict whether there is a boarded up or broken window, inspired by Google images I downloaded from addresses considered poor or very poor in Des Moines. An option to look for in future years is an AI model that can look for multiple characteristics such as age and damage for roofs. Another we discussed this week is AI heatmaps which show where the AI is looking in the image to determine it’s guess. I really want to attempt this time permitting.\nNow for reading relief:\nI made this graph real quick just to visualize the counties which we are focusing on. Grundy Center is in Grundy County, New Hampton is in Chickasaw County, Independence is in Buchanan County, and Slater is in Story County.\n\nHere is a closer look at the fulcrum app we used to evaluate houses. The first image shows the different folders for each community, followed by the interface for a singular house evaluation.\n\n\n\nNext for the code I built on the skeleton from last week mostly following the code plan diagram I made 2 weeks ago(?). As of right now house_evaluator.py can read in a folder with images in it (as if it was one house from multiple sources). The program will use the house_present_classifier, clear_image_classifier, and multiple_houses_classifier models. These models evaluate whether each image has a visible house, whether the house is obstructed, and if there are multiple houses visible. Ideally we want to evaluate an image with a house, minimal obstruction, and only one house visible. Each model will remove unwanted images from the list in the hopes of choosing only the best image of the house to evaluate.\nIf no image remains the program returns that a better image is needed of the house. If multiple good pictures of the house remain the program will randomly select one. I hope to implement looking at the date of the images before randomly selecting but we do not have dates of images stored from any source yet.\n\nNext the remaining image will be ran through the vegetation, siding, and gutter models. As I said last week the gutter model is on hold until I get more images of poor gutters and spaces in the code are there for roof and window models for either next year or my free time. All of the models predict pretty poorly still because we need a lot more data on bad house images but they are predicting at a 35-45% confidence rate currently.\nAfter I got all of the models in the program properly I started to work on writing to CSV files. The intention was to print each attribute such as vegetation in it’s respective column based on address. I used the CSV library which turned out to take longer than it seems giving me issues such as wiping the file clean, not writing any values, then writing values to the leftmost row available after address. I finally got the attributes to print to the correct column but printed on every row instead of only one row based on address.\nSadat, our AI fellow, used Pandas and could correctly do the correct task in about 10 lines where it took my method about 40 lines of code. Long story short I am now implementing pandas to manipulate the csv files.\nThis code shows the program checking if the columns exist in the csv and if not adding the column. Then the values of each is wrote to the CSV file. After discussion with Sadat I added the model confidence percentage so that as we upload more accurate models we can see how well they are preforming and also give an average confidence for a final report.\n\n\nIn summary this program so far can take in a folder of images, choose the best one, evaluate it’s vegetation, siding, and gutters, then print these attributes to a csv at the same address as the input folder. This python Script was nearing 400 lines of code due to tests and old code I commented out to save. To run many images through this program I think the easiest path would to make a parent folder full of folders labeled by address and inside of the address folders are the images for each address. I did not want a 400 line for loop to iterate through this parent folder so my goal for this weekend is to make two scripts.\nThe first script will read in the files and iterate through them calling the second script each run to evaluate the images within and return house attributes. The first script will take these attributes and return them to a CSV file to the proper column and address. This will be essentially the same as what I have working now but it will be able to read and document many folders of houses rather than individual houses manually placed at a time. I will leave the current script alone since it works and make two completely new scripts to improve this method.\n\n\nToday was very productive, the entire program now works. city_evaluator.py can read in a parent folder which contains address folders, it then calls house_evaluator.py for each address folder. house_evaluator runs images through the AI models (still largely inaccurate) then returns attributes of the image detected. Having multiple files was accomplished by creating a function within house_evaluator then calling that function in city_evaluator.\n\n\nAfter loading in all the variables in city_eval I wrote to the csv by address. This method largely stayed the same from yesterday I just had to tweak it so that the name being printed to was iterated.\n\n\nIn my actual conclusion I will say this was a very successful week even though it was shortened. I plan to make a blog post just for my weekend project so that I don’t have to remember it next Thursday. I will attempt to make an AI heatmap so we can visually see why the models guess incorrectly (right now they suck because we barely have any data). A lot of people I have talked to think of object detection when I say AI heatmap so I will need to explain in depth why this is different. Also in the image above you can see gutter is FALSE and 0 confidence because I have still neglected making that model. I now have access to data maybe I can get it made next week. We also REALLY need to update the other models with the data we collected in Grundy Center, Independence, and New Hampton."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html#address-collection-webscraping-zillow-webscraping-google-maps-api",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html#address-collection-webscraping-zillow-webscraping-google-maps-api",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#address-collection-and-cleaning",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#address-collection-and-cleaning",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "First we need to go to Beacon to try and collect a list of addresses. The following link will bring you to Boone County and you need to scroll in on Ogden (West/center). Beacon Map View\nWe found a tool called Instant Data Scraper that you need for this process. An alternative to this is badly needed as you will see soon, but this application worked for the time being when we had no prior web scraping experience yet. DSPG students for summer of 2024, I beg you to try and scrape Beacon with your own spider before using this tool to save yourself from the cleaning you have to do.\nBelow I display an image of the Beacon interface then an image zoomed in on Ogden.\n\n\nNavigate to the buttons towards the top of the screen and find the cursor over a box. This is the select tool that we need to use for selecting properties. There is a limit of how many addresses you can grab at once so below is what it looks like if you try to grab every property at once:\n\nInstead we need to click on this button and select the polygon version.\n\nTo avoid the problem of too many addresses I typically select four segments of the town and merge them after exporting to csv files. As you can see below I used the polygon tool to select all parcels north of 216th Street(the large road below my box). The polygon selector has multiple points you can set, just make sure each parcel you want is within the shaded area or on the line then double click to select all parcels.\n\nNotice on the right hand side how all of the parcels you selected show up. This is where we want to scrape the information from. Open the instant data scraper tool from your extensions button (top right). Below you can see how the chrome extension automatically found the information to the right which now has a red marker around it. I deleted all of the columns in this interface except for resultitem because from our experience this holds the parcel number, address, and owner.\n\n\nGo ahead and export to a csv. The file is automatically saved as beacon.csv so go ahead and rename this file to ogden_n_216_st.csv. This is a temporary name just so you can easily find your Ogden files and know that the first grab was north of this road that we chose. Repeat this process until you have grabbed all of the parcels you desire, you have to close the instant data scraper and open it again for each new grab. Some towns I could scrape in one go while some took four cuts to collect all the data.\nNext we are simply merging the csv files. Choose one of the files to be the main one, then go csv by csv to copy and paste into the main csv file. You could make a script to do this for you but honestly it doesn’t take that long, a trick is when you press ctrl + shift + down arrow you can select the entire column. ctrl + down arrow will bring you to the bottom of the csv column in your main file. Leave out the top row that has resultitem. Also all of the boxes have - or #NAME? don’t worry yet there is more in these boxes they are just on newlines.\nRename the main file to ogden_addresses because this in this file we will clean the data then create address links. First in B2 place this function =TRIM(CLEAN(SUBSTITUTE(A2,CHAR(160),” “))). This will reformat the input text to be on one line without hidden characters. Usually your able to double click the bottom right of the cell and it will auto fill all the way down but most of these functions refuse to auto fill so you just need to grab the corner and drag to fill in B to be easily accessible text. Next copy all of the cells in column B and paste as text into column C.\nCopy column C into column D. Select all of column D, navigate to the Data tab, then text to columns in data tools. Select delimited then press next, deselect tab then select other and enter - into the box then select next, then finish. This will separate the data by parcel number, address, and owner. I deleted the owner name due to it being unnecessary information.\n\nFinally I noticed address needs to be trimmed so in a new column you can use the function =TRIM(A2) then paste the result as values back into the address column."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#creating-google-api-links",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#creating-google-api-links",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "What are the Google API Links? Here is an example of the beginning of an address from Grundy Center:\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nthis is followed by &key=(API Key). Pasting the entire link brings you to an image provided by Google street view if it exists. You could also use latitude and longitude coordinates instead of address but the image you get is not guaranteed to be a front image of the house. Google however, has a built in program to get the front of a house if you enter the address if it can find one.\nWe will continue to use the file from the previous section. First you need a url_start column to store the first half of the url which is always the same (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=). So in columns G, H, I, I have the url in the previous sentence, City (Ogden), then State (IOWA). Next we need to concatenate the full address with =CONCAT(F2, “,”, H2, ” “, I2). F2 is the house address, H2 is city, and I2 is State which results in 119 W SYCAMORE ST, OGDEN IOWA for my first address. Copy this column into the next and paste as values. Do not forget to try double clicking the bottom right of the cell to auto fill before dragging.\nIn the next column use =SUBSTITUTE(K2, ” “,”+“) which will replace all the white space with + which is neccesary for the link. Again paste as values into the next column. Next =CONCAT(G2,M2) will combine what is my url_start column and my full_address_+ column to get the entire url needed to run through my Google API scraper.Finally we need to place the values of this last column I named url_full into the first column so it is easily accessible by the python script.\n\nNow you can grab one of the links from the leftmost column and check that the link works. All I did was copy that part of the link into my browser, added &key=(API Key) and this was my result:"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#scrape-google-images",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#scrape-google-images",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "For this section you need to obtain a Google API key to scrape images from the Google street view API. You also need to download R studio or an IDE that can run R code. If you have access to the DSPG Housing repository there is a folder named complete links which has a grab_images.R file. Below is the code for grabbing Google images for Ogden, keep in mind downloading images takes a very long time many of the files of 3k photos have taken me upwards of an hour to download. I also do not know how to control where the image file is uploaded. My current directory is my blog and the images are uploaded there, I assume there is a way to change the directory in R studio.\nSome errors I ran into while scraping: Google does not like addresses that are combined such as 123/456 Main St and it will cause an error. Some addresses you pull will start with a # and Google will not accept this. Lines that have the #NAME? error I delete the row. Some addresses I pulled from Ogden did not have an address but just an owner name which caused my program to throw an error. If the address is empty or does not start with a number I delete the row. I manually deleted just over 100 rows from the Ogden set which had missing addresses and was blank or filled in with owner name. This is a good example of why a personalized scraper should be made because while the chrome extension is convenient and fast it pulls back many issues with the data which we may get by scraping ourselves.\n# Ogden\nog_data &lt;- read.csv(“~/GitHub/Housing/complete links/ogden_urls.csv”)\nurls_start &lt;- og_data[, 1]\nurls_full &lt;- paste(urls_start, “&key=”, sep = ““)\nurls_full_api_key &lt;- paste(urls_full, api_key, sep = ““)\n# creates folder and downloads all images\ndir.create(“ogden_google_images_folder”)\nfor(i in seq_along(urls_full_api_key)) {\n___ file_path &lt;- file.path(“ogden_google_images_folder”, paste0(“G_OG_”, og_data[i,6], “_.png”))\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n___ print(file_path)\n___ print(i)\n}\nThe only changes to the above code is that you must change the path to your CSV file, where og_data[, num] the number must be changed to the column in the CSV with the full url if it is not in the first column already and the second instance changed to where the address column is, names of image export files if you use a different city, source or city within paste0 (convention is source_city_address_ where source is G Google, Z Zillow, Etc and city is OG Ogden, G Grundy Center, etc), and finally you must add an api_key variable with your own api key. In R Studio you can run just the variable name and it will save into your environment. Make sure to delete the API Key from your code before pushing it somewhere public such as GitHub.\nSome common errors with the images coming in are as follows: If Google does not have an image it will give you an image does not exist image but this should be easily identified by our AI model through training. Quick note if you retrain the models I show later in this guide only put 2-3 of these error images as to not throw off the model, having 50 or so will make the model think that image has a very high correlation. Next I have had an odd issue where the same image will be used for multiple addresses. This is quite weird because I do not know where the image is getting pulled from. Maybe there is a default image that is getting pulled somehow? But it is not just one image for Grundy Center which has about 3,500 images we pulled I saw anywhere from 10-20% of the images were duplicates with different addresses. There were probably 8-10 different images that were duplicated for different images so this is a big problem that doesn’t always happen but is a mystery to me why this happens. But hey 80-90% good images is a passing grade."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-binary-image-classification-ai-model",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-binary-image-classification-ai-model",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "These are the two videos I used from Nicholas Renotte Video 1 Video 2. The first video shows how to set up Jupyter Labs while the second video explains how his model works. I copied his code but altered it a little for the binary models but quite a bit for the multi model classifications.\nTo be honest I think the model in the next section can handle binary scenarios but because I do not fully trust that theory so I am going to show the code for the binary models as it is a little different. When I made my first model I used Jupyter Lab which for an experienced programmer it wasn’t horrible to set up the environment but for my teams sake and future DSPG members we switched to Google Colab. Google Colab is an online resource that can be edited by multiple users just as other Google applications.\nAccess to entire projects are available through our AI Housing team GitHub but I will walk through the house_present.ipynb file. I highly recommend going to our Housing repository in models_algorithm to look at models rather than this guide (download and open in Google Colab) as I think it is much easier to read and follow. First off, ipynb is the extension for Jupyter Notebooks which is python code written in individual cells so that you can run cell by cell rather than the entire program. This is helpful for testing and minor fixes but I suppose these can be written in normal .py (python) files.\nMake sure tensorflow, opencv, and matplotlib are downloaded. You can download these libraries with pip install tensorflow. !pip list will display a list of downloaded libraries to check if these are properly downloaded. This next step is vital for all Colab codes:\nfrom google.colab import drive drive.mount(‘/content/drive’)\n%cd “/content/drive/MyDrive/Colab Notebooks/house_present_model”\nThe from section will connect Google drive to Colab. I will explain where images go in the Training the Model section below but keep in mind you will need to download your images to Google drive to train the model. The third line of code is the path to my house_present_model within Google drive so you will need to replace that path with your path to the folder which holds the Google Colab ipynb file and data images. You do not have to worry about this for now follow along with the rest of the code then skip to Sorting Images then Training the Model section to run the code.\nI have my imports scattered through the rest of the document but I will list them all now:\nimport tensorflow as tf\nimport os\nimport cv2\nimport imghdr\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.metrics import Precision, Recall, BinaryAccuracy\nfrom keras.models import load_model\nAs you can see these models are built with Tensorflow and Keras. Next we grab the images which will be in the data folder.\ndata_dir = os.getcwd()+‘/data’\nWe want to remove bad images from our image dataset. First go through and delete all images 10 KB and smaller, then delete any files that are not images.\nimage_exts = [‘jpeg’,‘jpg’, ‘png’]\nfor image_class in os.listdir(data_dir):\n___ for image in os.listdir(os.path.join(data_dir, image_class)):\n___ ___ image_path = os.path.join(data_dir, image_class, image)\n___ ___ try:\n___ ___ ___ img = cv2.imread(image_path)\n___ ___ ___ tip = imghdr.what(image_path)\n___ ___ ___ if tip not in image_exts:\n___ ___ ___ ___ print(‘Image not in ext list {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\n___ ___ ___ except Exception as e:\n___ ___ ___ ___ print(‘Issue with image {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\nAt this point most of the images we have will be fine to use for training the model. To see a sample of the images you can use plt.imshow(img) or plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) followed by plt.show() to see a properly colored version.\nNext we need to separate the data into 0 or 1. The house_present model for example, I have two folders, one with images with no house and one with house images. The model will predict a decimal between 0 and 1 so if the model predicts .79 it will belong to the second category of house images. Below will show a sample of what this looks like.\ndata = tf.keras.utils.image_dataset_from_directory(‘data’)\ndata_iterator = data.as_numpy_iterator()\nbatch = data_iterator.next()\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\nfor idx, img in enumerate(batch[0][:4]):\n___ ax[idx].imshow(img.astype(int))\n___ ax[idx].title.set_text(batch[1][idx])\n\ndata = data.map(lambda x,y: (x/255, y))\ndata.as_numpy_iterator().next()\nNext we separate the data into training, validation and testing which should be separated by 70%, 20%, 10%. len(data) will show how many images will be used in each train. For my last run I had 7 images.\ntrain_size = int(len(data).7) val_size = int(len(data).2)+1 test_size = int(len(data)*.1)+1\ntrain_size+val_size+test_size\nThe last line should be equal to len(data) if test or validation is 0 you need to add numbers to the end as I did above. There is a better solution in the following AI model section that I will replace this with if I remember. Next we build the layers of the Neural Network.\nmodel = Sequential()\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’, input_shape=(256,256,3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(32, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=‘relu’))\nmodel.add(Dense(1, activation=‘sigmoid’))\nmodel.compile(‘adam’, loss=tf.losses.BinaryCrossentropy(), metrics=[‘accuracy’])\nmodel.summary()\nUsing the model that was just created now it needs to be trained.\nlogdir=‘logs’\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val.cache().prefetch(buffer_size=AUTOTUNE)\nhist = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[tensorboard_callback])\nNext we graph the loss and accuracy of the model.\nfig = plt.figure()\nplt.plot(hist.history[‘loss’], color=‘teal’, label=‘loss’)\nplt.plot(hist.history[‘val_loss’], color=‘orange’, label=‘val_loss’)\nfig.suptitle(‘Loss’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\nfig = plt.figure()\nplt.plot(hist.history[‘accuracy’], color=‘teal’, label=‘accuracy’)\nplt.plot(hist.history[‘val_accuracy’], color=‘orange’, label=‘val_accuracy’)\nfig.suptitle(‘Accuracy’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\n\nThen we can evaluate the quality of the model\npre = Precision()\nre = Recall()\nacc = BinaryAccuracy()\nfor batch in test.as_numpy_iterator():\n__ X, y = batch\n__ yhat = model.predict(X)\n__ print(yhat)\n__ pre.update_state(y, yhat)\n__ re.update_state(y, yhat)\n__ acc.update_state(y, yhat)\nprint(pre.result(), re.result(), acc.result())\nprint(f’Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}’)\nThen we test the model to see that it works visually. The directory in the following code refers to an image that I chose to test on in the same directory as the ipynb file and the data folder.\nimg = cv2.imread(‘/content/drive/MyDrive/Colab Notebooks/house_present_model/data/Copy of G_G_108 E AVE_.png’)\nresize = tf.image.resize(img, (256,256))\nplt.imshow(resize.numpy().astype(int))\nplt.show()\nyhat = model.predict(np.expand_dims(resize/255, 0))\nyhat\nif yhat &gt; 0.5:\n__ print(f’No house present’)\nelse:\n__ print(f’House present’)\nFinally we can save the model. Rename the h5 file to whatever model you are making and it will save to a new folder named models.\nmodel.save(os.path.join(‘models’,‘house_present_classifier.h5’))"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-multi-category-image-classification-ai-model",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#building-a-multi-category-image-classification-ai-model",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "This section is a review of the model that classifies an image when there are three or more categories. This model can also handle only two categories and may work better than the previous model. This model was somewhat based on the prior binary model but was just adjusted to handle more labels. This is also done on Google Colab so we start with mounting to Google Drive.\nfrom google.colab import drive\ndrive.mount(‘/content/drive’)\nThen we load in the images the same way. Make sure the images you will be using is in Google Drive in whatever directory you insert below.\n%cd “/content/drive/MyDrive/Colab Notebooks/DSPG Models/vegetation_quality_model”\ndata_dir = os.getcwd()+‘/data’\nWe then set the batch size and image size\nbatch_size = 32\nimg_height = 180\nimg_width = 180\ndata = tf.keras.utils.image_dataset_from_directory(\n__ data_dir,\n__ image_size=(img_height, img_width),\n__ batch_size=batch_size)\nWe then divide the data as 70% testing, 20% validation, and 10% testing.\ntrain_size = int(len(data)*.7)\nval_size = int(len(data)*.2)\ntest_size = int(len(data)-train_size-val_size)\nNext we show some of the images in the data set\n\nThis code makes the training process go very quickly\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val.cache().prefetch(buffer_size=AUTOTUNE)\nThen we build the neural network.\nnum_classes = len(class_names)\nmodel = Sequential([\n__ layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n__ layers.Conv2D(16, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Conv2D(32, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Conv2D(64, 3, padding=‘same’, activation=‘relu’),\n__ layers.MaxPooling2D(),\n__ layers.Flatten(),\n__ layers.Dense(128, activation=‘relu’),\n__ layers.Dense(num_classes, activation=‘softmax’)\n])\nmodel.compile(optimizer=‘adam’,\n__ loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n__ metrics=[‘accuracy’])\nThen we train the model.\nepochs=10\nwith tf.device(‘/device:GPU:0’):\n__ history = model.fit(\n____ train,\n____ validation_data=val,\n____ epochs=epochs)\nThen we print the Accuracy and Loss\nacc = history.history[‘accuracy’]\nval_acc = history.history[‘val_accuracy’]\nloss = history.history[‘loss’]\nval_loss = history.history[‘val_loss’]\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label=‘Training Accuracy’)\nplt.plot(epochs_range, val_acc, label=‘Validation Accuracy’)\nplt.legend(loc=‘lower right’)\nplt.title(‘Training and Validation Accuracy’)\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label=‘Training Loss’)\nplt.plot(epochs_range, val_loss, label=‘Validation Loss’)\nplt.legend(loc=‘upper right’)\nplt.title(‘Training and Validation Loss’)\nplt.show()\nThen we evaluate the quality of the model.\npre = Precision()\nre = Recall()\nacc = Accuracy()\nfor batch in test.as_numpy_iterator():\n__ X, y = batch\n__ yhat = model.predict(X)\n__ yhat = np.array(tf.math.argmax(yhat,axis=1))\n__ print(y)\n__ print(yhat)\n__ pre.update_state(y, yhat)\n__ re.update_state(y, yhat)\n__ acc.update_state(y, yhat)\nprint(f’Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}’)\nmodel.save(os.path.join(‘models’,‘vegetation_quality_classifier.h5’))\nAgain this is much easier to just go into the GitHub for housing and find these models. By the time of finishing this section I now have two more models that produce SHAP and CAM images which take multiple labels to go find towards the end of this project."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#sorting-images",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#sorting-images",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "This step follows the Google scraping section directly. Sorting images is a very time consuming process and we have not found a quicker way than manually sorting image by image. I have tried to make this process as efficient as possible with the following method. First make a copy of the image folder so that you have the original images if you need to sort multiple ways. Next go to the image folder and make folders for each sorting method you have. Below is an example of what it looked like while I was sorting for siding. I had a good siding, chipped paint, and poor folder which would be used for training the model. I also had an excess good folder and a delete folder. The excess good folder took a bulk of the good houses because you do not want your data amounts to be too uneven. If you have too many duplicates or too many images in one folder this can cause inaccuracies in the model’s predictions. The delete folder made the sorting go quicker to just throw images into that folder and delete them later rather than deleting bad images as they come.\n\nMake a new folder named data and put all of the categories into that folder. The data folder needs to go into the same directory as the ipynb file in google drive. As shown above the data folder needs to be accessible by Google Colab so that it can determine amount of categories and sort accordingly."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#training-the-model",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#training-the-model",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "If you have a ipynb file set up to create a model and you have sorted images into their respective folders, this step is easy. The following image should be similar to how your directory should look. After linking your Google Colab this left panel will show up with your folders, it is necessary for your data folder to be in the same directory as your ipynb file in Google Drive.\n\nOnce your sure the images are accessible for the model all you have to do is run all of the cells. There is an option to run all but I recommend running cell by cell to address errors as they appear. The final line will return the trained model into a new folder named models. You can download the h5 file (the trained model) from Google drive to your local machine to use them in the algorithm I wrote to evaluate images."
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#utilizing-a-trained-ai-model",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#utilizing-a-trained-ai-model",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "Make sure you have an IDE that can run python code. I recommend Visual Studio as it can run many languages and has libraries to make python look nice but pycharm is another popular python IDE.\n\n\nBefore using the python scripts to run your models we need to sort the images from the original Ogden folder into a format that the function can understand. The way I planned the storage of images is as follows: there is a parent folder that holds all images, within the parent folder there are folders for each individual city, and within city folders are folders named by all of the addresses in those cities, and finally within the address folders are images for each address from different sources. Inside of the housing channel folder models_algorithm exists a file image_folder_sorter.py. Each city that I sorted has it’s own algorithm for sorting because they are in different folders going to different folders. I will create one for Ogden below.\nimg_loc = os.path.expanduser(“~/Documents/downloaded google images/ogden_google_images_folder”)\nparent_folder = os.path.expanduser(“~/Documents/parent_folder_holder”)\naddress_folders = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\nfiles = os.listdir(img_loc)\nfor img in files:\n__ address = img.split(“_“)[2].strip()\n__ new_address_folder = os.path.join(parent_folder, address_folders, address)\n__ os.makedirs(new_address_folder, exist_ok=True)\n__ source_path = os.path.join(img_loc, img)\n__ destination_path = os.path.join(new_address_folder, img)\n__ shutil.copyfile(source_path, destination_path)\nAs shown above this algorithm grabs the Ogden image folder then copies it to another folder in which it is sorted by address (which is given by name of each image). Make sure to rename the directories if you have different folder names.\n\n\nI hope the following graphic is helpful in understanding how the images are stored.\n\n\n\n\nIn this section I will explain how to use your AI model to evaluate addresses purely through using my python scripts.\n\n\nI have two files, city_evaluator.py and house_evaluator.py. city_evaluator reads in images from the method stated above, it navigates to a parent folder which is full of cities, you can navigate to one of these cities to evaluate the images present in it’s folders. For each address folder there may be multiple images of the same address but each of these folders are fed to house_evaluator.\nhouse_evaluator has three models at the beginning, house_present which identifies if there is a house present in an image, clear_image which identifies if there is a clear image of the house (whether it is obstructed or clear), and multiple_houses which identifies if there are multiple images in the picture. These image models were made with the intention of filtering out bad images. If no houses remain the program returns with everything as false. If two good images exist the program randomly selects one of the two.\nIf an address has an identified good image we will then run it against all our attribute models. Currently there is a vegetation model, siding model, gutter model, and roof model. All of these models need to be better trained to become more accurate but right now serve as good examples of how to build and use AI models for this project. The ipynb files are located in the same location as the h5 files which can be downloaded and opened in Google Colab.\nFinally the program returns multiple variables to city_evaluator including whether there is a clear image, name of the test failed if it did, name of image used, whether image was randomly selected, vegetation, siding, gutter, and roofing predictions along with percent confidence. I will discuss how city_evaluator writes this information to a CSV file in the final section.\n\n\n\nI’m going to list everything you can alter as we go through adding the model here in case anything needs adjusted. This is starting in the house_evaluator.py file. First this screenshot is of the image names and image variable holders being initialized. If you gather images from sources other than what is labeled below you will need to add a variable everyplace these show up. This shouldn’t be too difficult, I only count four locations that you would need to add an image holder variable and a name variable.\n\nYou can skip past all of the house image quality models to what is now about line 220 which is right after the random picker operation. Lets say your new model identifies window quality, if no image remains then you need to add your variable to the list of variables returned here with windows as None and window confidence as 0\n\nNext scroll past all of the next models (vegetation, siding, gutter, and roof) I left a spot for your window model if this isn’t a hypothetical situation :). Whether your model is binary or has multiple labels changes how you must implement it. For a binary model you can look at the house_present model and the multiple_house model as an example while for a multi option model you can look at any of the attribute models. Below are screenshots of house_present followed by the siding_model, house_present may be misleading but keep in mind it needed to check if an image was present and delete or add so your attribute model will be altered a little from this code and more similair to the start and end of the siding model.\n\n\nFinally add your models variables to the final return statement.\n\nNavigate to city_evaluator.py and update the three spots where it handles variables. This should be simple just copy and paste the lines and change the variable name. An example of the code snippets below: 1. clear_image = attributes.get(“clear_image”)\n\n\n\nif (‘clear_image’ in list(df.columns)):\n__ pass\nelse:\n__ df[‘clear_image’] = None\n\ndf.at[i,‘clear_image’] = clear_image"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#exporting-predictions-to-a-csv-on-address",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#exporting-predictions-to-a-csv-on-address",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "city_evaluator.py already does the work for you, you just need to add links. First double check that all the variables mentioned in the last section is updated or the new values will not be pulled and written. The folder paths you need to change is the path to the parent folder that contains all of the images (below I show Ogden’s links), then you need to make a csv to hold all of your information. I do not think there is a generic database csv file we have but all of the categories can be copied from an existing one. At minimum the csv file must have all of the addresses from the csv file used to create Google urls or nothing will print.\n\nThen update the following lines with your city info and place them with the others in city_evaluator. You can see I just uncomment whichever city I want to run.\nmain_folder = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\ndf = pd.read_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’))\ndf.to_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’), index = False)\nNow the fun part. If everything worked correctly all you have to do is run city_evaluator and wait for quite a while for all the images to be processed. The terminal will look like below for while it runs through the models.\n\nOgden was smaller so ran a little faster but here is the final result of the current models I have evaluating Ogden:"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html#conclusion",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html#conclusion",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "I think this method goes pretty quick overall. Scraping addresses and cleaning goes quick once you have the functions to clean the data, AI models take a while to understand at first but after one or two the process becomes quicker, and implementing models and printing to CSV files take a while to code the first time but alterations are easy. The worst part of this process is sorting images for models. It can take hours and you may have to do it multiple times to increase the accuracy of models.\nSteps for the rest of this project not included in this guide:\nI will implement a roof model\nI will better train the current models\nBut the best part to go find once it’s done is AI heatmaps which are images that show where in an image a computer is most influenced to make a prediction."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#how-ai-works",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#how-ai-works",
    "title": "Guide: AI Models",
    "section": "",
    "text": "First an intro to the general idea of how image classification artificial intelligence models work. Image classification models use deep learning to analyze and classify images into different categories. These models typically use convolutional neural networks, or CNNs, which are designed to handle visual data. Computers cannot see images the same way as us, they must look over images in multiple ways to understand them. A CNN can looks at neighborhoods of pixels to look at edges, textures, and corners to find common patterns in a set of images.\n\n\nCNNs consist of multiple different layers which make up the neural network. After constructing a network we are able to ‘train’ the model on images. Our team used supervised learning to train the models, this simply means that we had pre-sorted categories for the model to learn from. A common saying is in this area is ‘garbage in, garbage out’, meaning if you feed the model bad data it will preform poorly. Sorting images for training models is a long process but it is necessary so that we can have the best output possible.\n\n\n\nNeural Network\n\n\nAfter a model is trained with data we can use it to predict which class new images are part of. For example if we train a model on images that have a picture of a house and images that have images of no house the model will predict whether a new image it has not seen before to have a house or to have no house."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#training-images",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#training-images",
    "title": "Guide: AI Models",
    "section": "",
    "text": "Training images are the most important factor of how well a model will perform. We cannot tune the model to be better preforming if the input images for training are poorly chosen. This is a huge issue the AI Housing team ran into this summer, there are plenty of images available online of good houses but much less of houses with damages which we are trying to evaluate. The following numbers are the ratios of images for the house present, vegetation, siding, gutter, and roof models.\n430:100, 200:40:170, 210:150:100, 200:130, 260:150\nOverall the ratios of images are not too bad but the total numbers are bad. For every model the smallest number is the ‘negative’ attribute whether that is a damaged gutter or damaged roof etc. Early models built during this project used about 400 images in each category just to test how models work and they predicted somewhat decent. Having less than 200 images to train on was very problematic because with only training on a couple hundred images there is no way a model can correctly predict thousands of new images from this data. This has greatly affected the model accuracy for every model we built. Future DSPG AI Housing team members need to sort many more images to increase the accuracy of models created before they can be considered reliable."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#google-colab",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#google-colab",
    "title": "Guide: AI Models",
    "section": "",
    "text": "Google Colab is an online Jupyter Notebooks Google service. The first pilot models built for this project was done on Jupyter Labs following these videos(Video 1 Video 2). Google Colab serves the same purpose but is more user friendly to students not as familiar with code, as there is less to set up. Colab pairs with Google Drive so all data being used must be saved there first then exported if it is needed somewhere else. Google Colab is where majority of our models were built."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#image-classification-models-built-with-tensorflow",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#image-classification-models-built-with-tensorflow",
    "title": "Guide: AI Models",
    "section": "",
    "text": "The first models we built used the Tensorflow and Keras libraries. If you are interested in seeing the program we wrote navigate here and all of the model folders have ipynb files which can be opened in Google Colab. But a short recap, we used 70% of the data for training, 20% for validation, and 10% for testing. The training data trains the model, the validation data validates that the model is training properly and not overfitting, and the testing data is used to test how well the model preforms after being trained. Next we can look at a sample of the data to see the different buckets that we sorted the images into.\n\nNow we must build the neural network, using a sequential model we used relu activation layers, max pooling layers, and a softmax dense layer.\n\n\nAfter building the network you can train based on the input data. This is the siding model we are walking through which has three classes of data input, good siding, siding with chipped paint, and poor or damaged siding.\n\nAfter the model is trained we can gather the accuracy and loss of the model. Accuracy is how accurate the model was at predicting classification of the testing data set while loss is the measure of how different a models predictions were from the actual labels. We want the accuracy as high as possible and loss as low as possible. Below an ideal loss and accuracy graph is shown, then the one for the siding model, then what overfitting looks like.\n\n\n\nOn a range of zero to one accuracy should be as close to one as possible while loss should be as close to zero as possible. The first image shows how these graphs should look to know you have a decent model. The second image shows what our graphs look like from the siding model. There are multiple possible issues that could be causing our graphs to look that way but the most probable reason right now is insufficient training data. Greatly increasing the training data will not perfect the accuracy and loss but this will certainly take us a step in that direction.\nThe third image shows another problem to watch out for once our model increases in quality. Overfitting is a phenomenon that occurs when a model becomes exceptional in the training data but cannot predict new images well. This can be avoided by increasing the data size or reducing model complexity. This is not yet a problem for any of the models used during this project but is an important concept to keep in mind for the future. Finally the model can be exported to be used for predictions."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#how-did-we-use-these-models",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#how-did-we-use-these-models",
    "title": "Guide: AI Models",
    "section": "",
    "text": "Below is an image of how our program works as a whole.\n\nThe AI Housing team built seven models for evaluating addresses (more were made this is discussed later). We made three models for image quality testing, the house present model which checks if a house is present in the image, the clear image model which checks to see if the house is obstructed, and the multiple house model which checks if multiple houses are visible.\nThe image quality models were made to eliminate bad images and select the best image to evaluate when given a few. Images with no houses or houses that can barely be seen due to obstruction should not be evaluated and if multiple images are available we want the image with only one house visible. After running through the first few models, if there are still multiple images remaining the program randomly selects one to evaluate.\nNext are the attribute models, which we currently have four, vegetation, siding, gutter, and roof. The vegetation model determines if there is no garden, a garden present, or overgrown weeds and bushes. The siding model determines if a house has good siding, chipped paint, or damaged siding such as panels missing or cracks. The gutter model determines if a house has good gutters or damaged gutters and the roof model determines if a house has a good roof or a damaged roof. Once all these attributes are predicted by the models we return them to a csv file joined on the address that was evaluated. We also keep track of the confidence percentage of each prediction which can be used for evaluation of the models confidence vs accuracy."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#models-evaluated-using-shap",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#models-evaluated-using-shap",
    "title": "Guide: AI Models",
    "section": "",
    "text": "SHapley Additive exPlanations or SHAP is a technique to explain predictions made by machine learning models. This roots from the Shapley value, which is a cooperative game theory that evaluates how important each player is for the overall cooperation and what payoff they can expect. This theory can be applied to machine learning models by looking at local accuracy, misingness, and consistency to determine what parts of input were most influential in the model making a prediction. Read more here.\nThe SHAP models were made in Google Colab using Tensorflow and keras just as the earlier models plus sklearn. As shown below the SHAP model is built with many more convolutional layers but as a result the loss is very low and the accuracy is very high. The following screenshots are from the house present SHAP model.\n\n\n\nNext we show the SHAP images. What is interesting about these graphs is that the first image shows that the model is looking at the outline of the house while the second model is looking exclusively at the trees behind the house. Our best explanation is that there is not enough training data for the model to realize that it is looking for the house not the trees being partially cut off. Either way it is interesting to see graphs of which pixels of the image most greatly influence a models prediction.\n\n\nWe have just simply gotten these models to run properly, they are not yet implemented into any other code. The SHAP models need to be compared to our first models to see if there are accuracy differences and if one network works better than the other. Additionally this can be implemented into our program to return a folder of images for each address evaluated so that we can get an idea of what type of images need to be added to the training data."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#models-made-with-pytorch-and-analyzed-using-cam",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#models-made-with-pytorch-and-analyzed-using-cam",
    "title": "Guide: AI Models",
    "section": "",
    "text": "Class Activation Mapping or CAM is another way of displaying why a model made a prediction. Similar to the SHAP technique, CAM highlights important regions in an image that play a role in the models prediction making what looks like a heatmap. Read more here.\nThe first model we built for CAM used keras and ResNet50. To read more on ResNet go here. Residual Networks or ResNet are Deep Learning networks that basically allow skipping of convolutional layers if they harm the models accuracy. The first model also used ImageNet which is a dataset with millions of images used for training and evaluation computer vision models. So this model was a test of what was possible with using a prebuilt model and a large dataset. We randomly decided to choose panda images as a test to see how the program performed.\n\nThis had us very excited to use our own model and dataset to try and get the same result as the image above. Quickly it was discovered that the upwards of fifty layers in resnet could not be easily replaced by our eight layer models. So we decided to try and learn PyTorch (an alternative library to Tensorflow).\nThe torch library has many different functions than Tensorflow and the neural networks are constructed a little differently. Thanks to courses on DataCamp we were able to figure out how to use PyTorch and get some good results.\nThe second model built was a house presence model. We put this code in a python file instead of a jupyter notebooks file like what is produced in Google Colab just to test out the difference. The process of using our own training data and producing CAM images was a surprisingly quick process with minor hiccups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove is the progression of the house present CAM model. At first the heat bubbles showed up in what seemed to be random locations but what was a mystery is that they were the same in all test images. It turned out that the program was overriding the variable keeping track of what the model was doing for the prediction rather than making copies of the information.\nAfter this was fixed, the model started to show certain areas that it was looking at to make it’s prediction. We were not satisfied with the images because they still seemed to be random. At this point we had 400 images of houses and only 40 images of no house present so we spent time to almost double the amount of images with no house present to improve the model. After rerunning the program more heat spots were visible and seemed a little less random.\nNote two things, first it seems this model likes to look at the trees for it’s prediction just as the SHAP model and after more data was added the left image was correctly predicted no house present.\nHowever we were still not satisfied this being the last week there needs to be more to show that this program works than random blotches. So we took the model and replaced the data with 200 images of cats and dogs to see if the model was really working properly.\n\n\nAbove you can see the first image looks at the nose and some of the fur. We doubled the training data which resulted in the second image which has more red around one of the eyes. Going back to the panda image earlier, the more accurate the model is the more red we will see around the object that it identifies. This last cat image is so exciting to see because unlike the house images that seem to be random blotches, most of the color is concentrated on the cat’s face."
  },
  {
    "objectID": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#conclusion",
    "href": "posts/Gavin-Fisher_AI_guide_w9/AI_guide.html#conclusion",
    "title": "Guide: AI Models",
    "section": "",
    "text": "Overall messing around with AI is pretty interesting, the worst part is definitely sorting training images for your models, but once you have a training set it’s fun. To reiterate it is much easier to go look at the code for the AI models to see what is happening. We got the models to predict correctly about 80% of the time and wish next years housing team luck on improving these models further."
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html#ai-model-project-plan-and-review-of-websites-to-be-scraped",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html#ai-model-project-plan-and-review-of-websites-to-be-scraped",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using Iowaassessors.com I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html#created-skeleton-code-for-the-ai-models-algorithm",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html#created-skeleton-code-for-the-ai-models-algorithm",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. I was responsible for the new vegetation model and siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example, for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels. The new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\nWe also went to the ITAG conference. I went to Bullwall by Don McCraw, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule. I found the Bullwall talk very interesting which explained generally how cybersecurity works and how their product is an extra protection layer."
  },
  {
    "objectID": "posts/Gavin-Fisher_week7/Week_Seven.html#image-storage-and-database-updating",
    "href": "posts/Gavin-Fisher_week7/Week_Seven.html#image-storage-and-database-updating",
    "title": "Week Seven Blog",
    "section": "",
    "text": "First I had to resort Google images into folders that could be processed by my city_evaluator.py file. I made a diagram to show how the images are being stored below.\n\nI made an image sorting algorithm that takes a source such as Google’s images then sort each image based on address and make a folder for each address. The reason each town folder has folders with address names is so that when I pull in winvest images as I do below I can have multiple images for the same address to run through the AI models. The next images just show the parent folder and how the folders are stored same as the above graphic.\n\n\nAn update from last week was that instead of printing to a test file I am now able to update each CSV with information needed. I added a test_failed category to show which image quality model kicked the address back and I added the title of the image used for the evaluation. Also added is the gutter model and a space for the roofing model. The first image below shows what happens when no image is found for an address. The second is a picture of New Hampton being evaluated. The reason it is missing multiple values is because the Google streetview had a lot of missing images for New Hampton. I added a third image which is Independence which is filled out from the later part of the week so it is full and has more attributes.\n\n\n\nI made a full guide of how to recreate the results of this project code wise here. I plan to do more before the summer finishes but this guide covers how to scrape images, clean data, make urls, scrape the Google API for streetview images, make models, implement models, and finally export results of models to a CSV for each address. Our team also made the teaser video for our AI housing project.\nI have also made an algorithm that will rename photos from Winvest so that I can sort it into the parent folder and have more than just Google images to evaluate. A lot of this week has been updating and improving the code from last week so that everything works better. On my todo list includes adding the roof model, retrain existing models to improve accuracy, and try to make heatmaps by the end of the project."
  },
  {
    "objectID": "posts/Gavin-Fisher_week9/Week_Nine.html#guide-cam-box-presentation",
    "href": "posts/Gavin-Fisher_week9/Week_Nine.html#guide-cam-box-presentation",
    "title": "Week Nine Blog",
    "section": "",
    "text": "This is our final week with our final presentation on Friday.\nFirst I made a guide on AI used during this project here. This guide is for the Friday presentation but also future DSPG students.\n\n\nI am so glad to announce that I got the CAM model working as of Monday morning. It turns out that in the weekend Seven blog I almost had the entire model running properly. What I found during this eighth weekend is that instead of tracking the weights as the model predicted I was rewriting the value. So it was a simple fix of adding .clone() to make clones of the x variable. Last week it seemed that the heat map marks were showing up randomly and was the same in every test image but after my tweak they seemed random still but now different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference between the second and third row of images is the amount of data. As I increased the amount of data the accuracy of the model increased with it so the red spots increased in size. I realized that the model may preform better if I used it on different data that was more simple such as cats and dogs. I think the reason the house present model is struggling in prediction is due to the trees in the background but as data increases accuracy should too.\nI duplicated the model to train on cats and dogs instead to see how well it worked when more simple data was available. The house present model had a ratio of about 400 images of houses present and 40 images of no house present which is a bad ratio while dog and cat images from Google I started with 100 of each category then increased to 200 in each category.\n\n\nI was very excited to see these images because it showed that the CAM model was working properly. The house present category is difficult to determine due to trees and we allow houses to be in the background if it is a picture of an empty lot but it preforms much better on images of cats and dogs. With just 200 images of cats the model was able to look at the eyes and nose to determine that it was a cat. The second image has more coloring because it had more training data. An image that would show very high accuracy and confidence can be shown with the model I used to evaluate pandas during the 6th weekend here.\n\nNext I downloaded all of the images we have so far into box to replace the messy images that were there. I have made programs to download new Google Images and sort them so if I deleted extra images next year can download new ones. I will delete old Google images and upload our training sorted images by the end of the week.\nSome time was spent writing recommendations for next year. I wrote most of the code and model recommendations, I hope these blogs will be helpful to get the next team started on the project.\nHuge issue I just found, the h5 file produced for the SHAP model is too big to upload to GitHub. The maximum is 100 MB and the h5 file is 197 MB. I decided that this is not my problem (sorry next year) but I uploaded the code to construct new models. There is a process to upload large files through git here but I am not sure if other people in the project also need to do this so I just left the model out.\nThis is the final information I will ever put in this blog :). Today is the last day of work all we did was look over the presentation and edit some things. Below I’ll post the updated graphics I made. Adios hope you enjoyed reading through this blog."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html#cam-shap-and-learning-about-ai-architecture",
    "href": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html#cam-shap-and-learning-about-ai-architecture",
    "title": "Weekend Seven Blog",
    "section": "",
    "text": "During this long weekend I learned more about AI models and toyed around with the heat maps I wanted to get done. The SHAP method mostly works and the CAM method prints but does not work quite yet and I will show both of these below.\nFirst I finished a course in DataCamp labeled Deep Learning with PyTorch. This course gave me a lot more insight on how to build layers of Neural Networks and gave some cool graphics I wanted to display after my ramble.\nI learned about some of the math behind machine learning such as matrix multiplication, derivatives, and how weights are set to each connection of the network.\n\n\n\n\nNext I made an AI model that prints SHAP images from it’s prediction. I used the gutter model first then made another for house present. Currently the image is fed to the AI too blurry for it to see the gutter so I need to go back and give the AI a more detailed image to evaluate but in the meantime I made another model for the house present data.\nThese models use keras and sklearn to build the network just as our previous models and then uses the shap library to create images that show basically what the AI is using to make it’s prediction. These models were also made in Google Colab.\n\n\n\n\nAs you can see I tried to display the data a few different way’s and they definitely need to be tweaked. The last three images are the SHAP images, my favorite is the last one. It is showing all of those empty graphs for some reason but the actual images shown is what we want. It shows which pixels the model is looking at to make each determination. You may notice the two images are basically the same but this makes sense because if the model determines one line of pixels shows a house is probably present the model will use the same pixels to determine that the no house present category is false.\nFinally I tried to apply what I learned about PyTorch to make a CAM model. I made this model in a Py file just to see the difference of running in Google Colab. This model uses torch, torchvision, and sklearn.\n\n\nI got the image to display but as you can see for an example of a house present and no house present the heat map is exactly the same which probably means it is not tracking the neural network properly.\nI think this all is a good step forward and hopefully was enough to finish this by our presentation next week.\nAlso it’s Independence Day, God Bless the USA"
  },
  {
    "objectID": "posts/Housing-Week-Eight/House_Week_Eight.html#ai-housing-week-eight-recap",
    "href": "posts/Housing-Week-Eight/House_Week_Eight.html#ai-housing-week-eight-recap",
    "title": "Housing Team Recap Week Eight",
    "section": "",
    "text": "Shortened blog this week.\n\n\n\nFinished editing demographic profile plots\nState, regional, and national context added to graphs\nAdded percentages to some of the graphs\n\n\n\n\n\n\nCreated house attribute quality dashboards\nMade a guide on how to create the maps\nPlan to make graphs on AI model accuracy vs actual house quality\n\n\n\n\n\n\nThanks to Aaron’s spider, Beacon images were added to the Independence database evaluation\nSHAP model works on our own datasets (Sadat fixed the code)\nPlan to have SHAP models for each attribute to visualize how well the models are preforming\n\n\n\n\n\n\n\nFinish demographic analysis for Iowa communities\nGraph AI confidence vs accuracy\nImplement SHAP models into program (and hopefully CAM too!)"
  }
]