[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Gavin is from Bolingbrook Illinois, an upcoming Junior at Iowa State University with a major in Computer Science and a minor in Data Science. He is part of the Cardinal Space Mining Club as part of the controls team and soon to be treasurer of the club. This blog’s purpose is to track Gavin’s work done at DSPG as part of the AI housing team.\n\nLinks to other members of the DSPG AI Housing Team\nUndergrads: Angelina, Kailyn\nTeam leads: Morenike, Sadat\nDSPG Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gavin’s DSPG Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWeekend Seven Blog\n\n\n2 min\n\n\n\nWeek Seven\n\n\nAI Models\n\n\nPython\n\n\nWeekend\n\n\nGoogle Colab\n\n\nPyTorch\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJuly 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Seven Blog\n\n\n1 min\n\n\n\nWeek Seven\n\n\nPython\n\n\nExcel\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuide: How to Evaluate a City\n\n\n22 min\n\n\n\nWeek Seven\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\nR\n\n\nGuide\n\n\nWeb Scraping\n\n\nGoogle Colab\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Six Blog\n\n\n1 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nWeekend\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Six Blog\n\n\n6 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Five Blog\n\n\n2 min\n\n\n\nWeek Five\n\n\nAI Models\n\n\nPython\n\n\nGoogle Colab\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Five\n\n\n3 min\n\n\n\nWeek Five\n\n\nHousing Team\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Four Blog\n\n\n3 min\n\n\n\nWeek Four\n\n\nWeb Scraping\n\n\nProject Plan\n\n\nExcel\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Three Blog\n\n\n1 min\n\n\n\nWeek Three\n\n\nAI Models\n\n\nProject Plan\n\n\nJupyter Notebook\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Two\n\n\n0 min\n\n\n\nWeek Two\n\n\nHousing Team\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Two Blog\n\n\n1 min\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek One Blog\n\n\n1 min\n\n\n\nWeek One\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "This will be a full guide on how to recreate what AI housing team has done this summer using Ogden IA as an example.\n\n\nFirst we need to go to Beacon to try and collect a list of addresses. The following link will bring you to Boone County and you need to scroll in on Ogden (West/center). Beacon Map View\nWe found a tool called Instant Data Scraper that you need for this process. An alternative to this is badly needed as you will see soon, but this application worked for the time being when we had no prior web scraping experience yet. DSPG students for summer of 2024, I beg you to try and scrape Beacon with your own spider before using this tool to save yourself from the cleaning you have to do.\nBelow I display an image of the Beacon interface then an image zoomed in on Ogden.\n\n\nNavigate to the buttons towards the top of the screen and find the cursor over a box. This is the select tool that we need to use for selecting properties. There is a limit of how many addresses you can grab at once so below is what it looks like if you try to grab every property at once:\n\nInstead we need to click on this button and select the polygon version.\n\nTo avoid the problem of too many addresses I typically select four segments of the town and merge them after exporting to csv files. As you can see below I used the polygon tool to select all parcels north of 216th Street(the large road below my box). The polygon selector has multiple points you can set, just make sure each parcel you want is within the shaded area or on the line then double click to select all parcels.\n\nNotice on the right hand side how all of the parcels you selected show up. This is where we want to scrape the information from. Open the instant data scraper tool from your extensions button (top right). Below you can see how the chrome extension automatically found the information to the right which now has a red marker around it. I deleted all of the columns in this interface except for resultitem because from our experience this holds the parcel number, address, and owner.\n\n\nGo ahead and export to a csv. The file is automatically saved as beacon.csv so go ahead and rename this file to ogden_n_216_st.csv. This is a temporary name just so you can easily find your Ogden files and know that the first grab was north of this road that we chose. Repeat this process until you have grabbed all of the parcels you desire, you have to close the instant data scraper and open it again for each new grab. Some towns I could scrape in one go while some took four cuts to collect all the data.\nNext we are simply merging the csv files. Choose one of the files to be the main one, then go csv by csv to copy and paste into the main csv file. You could make a script to do this for you but honestly it doesn’t take that long, a trick is when you press ctrl + shift + down arrow you can select the entire column. ctrl + down arrow will bring you to the bottom of the csv column in your main file. Leave out the top row that has resultitem. Also all of the boxes have - or #NAME? don’t worry yet there is more in these boxes they are just on newlines.\nRename the main file to ogden_addresses because this in this file we will clean the data then create address links. First in B2 place this function =TRIM(CLEAN(SUBSTITUTE(A2,CHAR(160),” “))). This will reformat the input text to be on one line without hidden characters. Usually your able to double click the bottom right of the cell and it will auto fill all the way down but most of these functions refuse to auto fill so you just need to grab the corner and drag to fill in B to be easily accessible text. Next copy all of the cells in column B and paste as text into column C.\nCopy column C into column D. Select all of column D, navigate to the Data tab, then text to columns in data tools. Select delimited then press next, deselect tab then select other and enter - into the box then select next, then finish. This will separate the data by parcel number, address, and owner. I deleted the owner name due to it being unnecessary information.\n\nFinally I noticed address needs to be trimmed so in a new column you can use the function =TRIM(A2) then paste the result as values back into the address column.\n\n\n\nWhat are the Google API Links? Here is an example of the beginning of an address from Grundy Center:\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nthis is followed by &key=(API Key). Pasting the entire link brings you to an image provided by Google street view if it exists. You could also use latitude and longitude coordinates instead of address but the image you get is not guaranteed to be a front image of the house. Google however, has a built in program to get the front of a house if you enter the address if it can find one.\nWe will continue to use the file from the previous section. First you need a url_start column to store the first half of the url which is always the same (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=). So in columns G, H, I, I have the url in the previous sentence, City (Ogden), then State (IOWA). Next we need to concatenate the full address with =CONCAT(F2, “,”, H2, ” “, I2). F2 is the house address, H2 is city, and I2 is State which results in 119 W SYCAMORE ST, OGDEN IOWA for my first address. Copy this column into the next and paste as values. Do not forget to try double clicking the bottom right of the cell to auto fill before dragging.\nIn the next column use =SUBSTITUTE(K2, ” “,”+“) which will replace all the white space with + which is neccesary for the link. Again paste as values into the next column. Next =CONCAT(G2,M2) will combine what is my url_start column and my full_address_+ column to get the entire url needed to run through my Google API scraper.Finally we need to place the values of this last column I named url_full into the first column so it is easily accessible by the python script.\n\nNow you can grab one of the links from the leftmost column and check that the link works. All I did was copy that part of the link into my browser, added &key=(API Key) and this was my result:\n\n\n\n\nFor this section you need to obtain a Google API key to scrape images from the Google street view API. You also need to download R studio or an IDE that can run R code. If you have access to the DSPG Housing repository there is a folder named complete links which has a grab_images.R file. Below is the code for grabbing Google images for Ogden, keep in mind downloading images takes a very long time many of the files of 3k photos have taken me upwards of an hour to download. I also do not know how to control where the image file is uploaded. My current directory is my blog and the images are uploaded there, I assume there is a way to change the directory in R studio.\nSome errors I ran into while scraping: Google does not like addresses that are combined such as 123/456 Main St and it will cause an error. Some addresses you pull will start with a # and Google will not accept this. Lines that have the #NAME? error I delete the row. Some addresses I pulled from Ogden did not have an address but just an owner name which caused my program to throw an error. If the address is empty or does not start with a number I delete the row. I manually deleted just over 100 rows from the Ogden set which had missing addresses and was blank or filled in with owner name. This is a good example of why a personalized scraper should be made because while the chrome extension is convenient and fast it pulls back many issues with the data which we may get by scraping ourselves.\n# Ogden\nog_data <- read.csv(“~/GitHub/Housing/complete links/ogden_urls.csv”)\nurls_start <- og_data[, 1]\nurls_full <- paste(urls_start, “&key=”, sep = ““)\nurls_full_api_key <- paste(urls_full, api_key, sep = ““)\n# creates folder and downloads all images\ndir.create(“ogden_google_images_folder”)\nfor(i in seq_along(urls_full_api_key)) {\n___ file_path <- file.path(“ogden_google_images_folder”, paste0(“G_OG_”, og_data[i,6], “_.png”))\n___ download.file(urls_full_api_key[i], file_path, mode = “wb”)\n___ print(file_path)\n___ print(i)\n}\nThe only changes to the above code is that you must change the path to your CSV file, where og_data[, num] the number must be changed to the column in the CSV with the full url if it is not in the first column already and the second instance changed to where the address column is, names of image export files if you use a different city, source or city within paste0 (convention is source_city_address_ where source is G Google, Z Zillow, Etc and city is OG Ogden, G Grundy Center, etc), and finally you must add an api_key variable with your own api key. In R Studio you can run just the variable name and it will save into your environment. Make sure to delete the API Key from your code before pushing it somewhere public such as GitHub.\nSome common errors with the images coming in are as follows: If Google does not have an image it will give you an image does not exist image but this should be easily identified by our AI model through training. Quick note if you retrain the models I show later in this guide only put 2-3 of these error images as to not throw off the model, having 50 or so will make the model think that image has a very high correlation. Next I have had an odd issue where the same image will be used for multiple addresses. This is quite weird because I do not know where the image is getting pulled from. Maybe there is a default image that is getting pulled somehow? But it is not just one image for Grundy Center which has about 3,500 images we pulled I saw anywhere from 10-20% of the images were duplicates with different addresses. There were probably 8-10 different images that were duplicated for different images so this is a big problem that doesn’t always happen but is a mystery to me why this happens. But hey 80-90% good images is a passing grade.\n\n\n\nThese are the two videos I used from Nicholas Renotte Video 1 Video 2. The first video shows how to set up Jupyter Labs while the second video explains how his model works. I copied his code but altered it a little for the binary models but quite a bit for the multi model classifications.\nTo be honest I think the model in the next section can handle binary scenarios but because I do not fully trust that theory so I am going to show the code for the binary models as it is a little different. When I made my first model I used Jupyter Lab which for an experienced programmer it wasn’t horrible to set up the environment but for my teams sake and future DSPG members we switched to Google Colab. Google Colab is an online resource that can be edited by multiple users just as other Google applications.\nAccess to entire projects are available through our AI Housing team GitHub but I will walk through the house_present.ipynb file. I highly recommend going to our Housing repository in models_algorithm to look at models rather than this guide (download and open in Google Colab) as I think it is much easier to read and follow. First off, ipynb is the extension for Jupyter Notebooks which is python code written in individual cells so that you can run cell by cell rather than the entire program. This is helpful for testing and minor fixes but I suppose these can be written in normal .py (python) files.\nMake sure tensorflow, opencv, and matplotlib are downloaded. You can download these libraries with pip install tensorflow. !pip list will display a list of downloaded libraries to check if these are properly downloaded. This next step is vital for all Colab codes:\nfrom google.colab import drive drive.mount(‘/content/drive’)\n%cd “/content/drive/MyDrive/Colab Notebooks/house_present_model”\nThe from section will connect Google drive to Colab. I will explain where images go in the Training the Model section below but keep in mind you will need to download your images to Google drive to train the model. The third line of code is the path to my house_present_model within Google drive so you will need to replace that path with your path to the folder which holds the Google Colab ipynb file and data images. You do not have to worry about this for now follow along with the rest of the code then skip to Sorting Images then Training the Model section to run the code.\nI have my imports scattered through the rest of the document but I will list them all now:\nimport tensorflow as tf\nimport os\nimport cv2\nimport imghdr\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.metrics import Precision, Recall, BinaryAccuracy\nfrom keras.models import load_model\nAs you can see these models are built with Tensorflow and Keras. Next we grab the images which will be in the data folder.\ndata_dir = os.getcwd()+‘/data’\nWe want to remove bad images from our image dataset. First go through and delete all images 10 KB and smaller, then delete any files that are not images.\nimage_exts = [‘jpeg’,‘jpg’, ‘png’]\nfor image_class in os.listdir(data_dir):\n___ for image in os.listdir(os.path.join(data_dir, image_class)):\n___ ___ image_path = os.path.join(data_dir, image_class, image)\n___ ___ try:\n___ ___ ___ img = cv2.imread(image_path)\n___ ___ ___ tip = imghdr.what(image_path)\n___ ___ ___ if tip not in image_exts:\n___ ___ ___ ___ print(‘Image not in ext list {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\n___ ___ ___ except Exception as e:\n___ ___ ___ ___ print(‘Issue with image {}’.format(image_path))\n___ ___ ___ ___ os.remove(image_path)\nAt this point most of the images we have will be fine to use for training the model. To see a sample of the images you can use plt.imshow(img) or plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) followed by plt.show() to see a properly colored version.\nNext we need to separate the data into 0 or 1. The house_present model for example, I have two folders, one with images with no house and one with house images. The model will predict a decimal between 0 and 1 so if the model predicts .79 it will belong to the second category of house images. Below will show a sample of what this looks like.\ndata = tf.keras.utils.image_dataset_from_directory(‘data’)\ndata_iterator = data.as_numpy_iterator()\nbatch = data_iterator.next()\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\nfor idx, img in enumerate(batch[0][:4]):\n___ ax[idx].imshow(img.astype(int))\n___ ax[idx].title.set_text(batch[1][idx])\n\ndata = data.map(lambda x,y: (x/255, y))\ndata.as_numpy_iterator().next()\nNext we separate the data into training, validation and testing which should be separated by 70%, 20%, 10%. len(data) will show how many images will be used in each train. For my last run I had 7 images.\ntrain_size = int(len(data).7) val_size = int(len(data).2)+1 test_size = int(len(data)*.1)+1\ntrain_size+val_size+test_size\nThe last line should be equal to len(data) if test or validation is 0 you need to add numbers to the end as I did above. There is a better solution in the following AI model section that I will replace this with if I remember. Next we build the layers of the Neural Network.\nmodel = Sequential()\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’, input_shape=(256,256,3)))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(32, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Conv2D(16, (3,3), 1, activation=‘relu’))\nmodel.add(MaxPooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=‘relu’))\nmodel.add(Dense(1, activation=‘sigmoid’))\nmodel.compile(‘adam’, loss=tf.losses.BinaryCrossentropy(), metrics=[‘accuracy’])\nmodel.summary()\nUsing the model that was just created now it needs to be trained.\nlogdir=‘logs’\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val.cache().prefetch(buffer_size=AUTOTUNE)\nhist = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[tensorboard_callback])\nNext we graph the loss and accuracy of the model.\nfig = plt.figure()\nplt.plot(hist.history[‘loss’], color=‘teal’, label=‘loss’)\nplt.plot(hist.history[‘val_loss’], color=‘orange’, label=‘val_loss’)\nfig.suptitle(‘Loss’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\nfig = plt.figure()\nplt.plot(hist.history[‘accuracy’], color=‘teal’, label=‘accuracy’)\nplt.plot(hist.history[‘val_accuracy’], color=‘orange’, label=‘val_accuracy’)\nfig.suptitle(‘Accuracy’, fontsize=20)\nplt.legend(loc=“upper left”)\nplt.show()\n\nThen we can evaluate the quality of the model\npre = Precision()\nre = Recall()\nacc = BinaryAccuracy()\nfor batch in test.as_numpy_iterator():\n__ X, y = batch\n__ yhat = model.predict(X)\n__ print(yhat)\n__ pre.update_state(y, yhat)\n__ re.update_state(y, yhat)\n__ acc.update_state(y, yhat)\nprint(pre.result(), re.result(), acc.result())\nprint(f’Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}’)\nThen we test the model to see that it works visually. The directory in the following code refers to an image that I chose to test on in the same directory as the ipynb file and the data folder.\nimg = cv2.imread(‘/content/drive/MyDrive/Colab Notebooks/house_present_model/data/Copy of G_G_108 E AVE_.png’)\nresize = tf.image.resize(img, (256,256))\nplt.imshow(resize.numpy().astype(int))\nplt.show()\nyhat = model.predict(np.expand_dims(resize/255, 0))\nyhat\nif yhat > 0.5:\n__ print(f’No house present’)\nelse:\n__ print(f’House present’)\nFinally we can save the model. Rename the h5 file to whatever model you are making and it will save to a new folder named models.\nmodel.save(os.path.join(‘models’,‘house_present_classifier.h5’))\n\n\n\nIn progress …\n\n\n\nThis step follows the Google scraping section directly. Sorting images is a very time consuming process and we have not found a quicker way than manually sorting image by image. I have tried to make this process as efficient as possible with the following method. First make a copy of the image folder so that you have the original images if you need to sort multiple ways. Next go to the image folder and make folders for each sorting method you have. Below is an example of what it looked like while I was sorting for siding. I had a good siding, chipped paint, and poor folder which would be used for training the model. I also had an excess good folder and a delete folder. The excess good folder took a bulk of the good houses because you do not want your data amounts to be too uneven. If you have too many duplicates or too many images in one folder this can cause inaccuracies in the model’s predictions. The delete folder made the sorting go quicker to just throw images into that folder and delete them later rather than deleting bad images as they come.\n\nMake a new folder named data and put all of the categories into that folder. The data folder needs to go into the same directory as the ipynb file in google drive. As shown above the data folder needs to be accessible by Google Colab so that it can determine amount of categories and sort accordingly.\n\n\n\nIf you have a ipynb file set up to create a model and you have sorted images into their respective folders, this step is easy. The following image should be similar to how your directory should look. After linking your Google Colab this left panel will show up with your folders, it is necessary for your data folder to be in the same directory as your ipynb file in Google Drive.\n\nOnce your sure the images are accessible for the model all you have to do is run all of the cells. There is an option to run all but I recommend running cell by cell to address errors as they appear. The final line will return the trained model into a new folder named models. You can download the h5 file (the trained model) from Google drive to your local machine to use them in the algorithm I wrote to evaluate images.\n\n\n\nMake sure you have an IDE that can run python code. I recommend Visual Studio as it can run many languages and has libraries to make python look nice but pycharm is another popular python IDE.\n\n\nBefore using the python scripts to run your models we need to sort the images from the original Ogden folder into a format that the function can understand. The way I planned the storage of images is as follows: there is a parent folder that holds all images, within the parent folder there are folders for each individual city, and within city folders are folders named by all of the addresses in those cities, and finally within the address folders are images for each address from different sources. Inside of the housing channel folder models_algorithm exists a file image_folder_sorter.py. Each city that I sorted has it’s own algorithm for sorting because they are in different folders going to different folders. I will create one for Ogden below.\nimg_loc = os.path.expanduser(“~/Documents/downloaded google images/ogden_google_images_folder”)\nparent_folder = os.path.expanduser(“~/Documents/parent_folder_holder”)\naddress_folders = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\nfiles = os.listdir(img_loc)\nfor img in files:\n__ address = img.split(“_“)[2].strip()\n__ new_address_folder = os.path.join(parent_folder, address_folders, address)\n__ os.makedirs(new_address_folder, exist_ok=True)\n__ source_path = os.path.join(img_loc, img)\n__ destination_path = os.path.join(new_address_folder, img)\n__ shutil.copyfile(source_path, destination_path)\nAs shown above this algorithm grabs the Ogden image folder then copies it to another folder in which it is sorted by address (which is given by name of each image). Make sure to rename the directories if you have different folder names.\n\n\nI hope the following graphic is helpful in understanding how the images are stored.\n\n\n\n\nIn this section I will explain how to use your AI model to evaluate addresses purely through using my python scripts.\n\n\nI have two files, city_evaluator.py and house_evaluator.py. city_evaluator reads in images from the method stated above, it navigates to a parent folder which is full of cities, you can navigate to one of these cities to evaluate the images present in it’s folders. For each address folder there may be multiple images of the same address but each of these folders are fed to house_evaluator.\nhouse_evaluator has three models at the beginning, house_present which identifies if there is a house present in an image, clear_image which identifies if there is a clear image of the house (whether it is obstructed or clear), and multiple_houses which identifies if there are multiple images in the picture. These image models were made with the intention of filtering out bad images. If no houses remain the program returns with everything as false. If two good images exist the program randomly selects one of the two.\nIf an address has an identified good image we will then run it against all our attribute models. Currently there is a vegetation model, siding model, gutter model, and roof model. All of these models need to be better trained to become more accurate but right now serve as good examples of how to build and use AI models for this project. The ipynb files are located in the same location as the h5 files which can be downloaded and opened in Google Colab.\nFinally the program returns multiple variables to city_evaluator including whether there is a clear image, name of the test failed if it did, name of image used, whether image was randomly selected, vegetation, siding, gutter, and roofing predictions along with percent confidence. I will discuss how city_evaluator writes this information to a CSV file in the final section.\n\n\n\nI’m going to list everything you can alter as we go through adding the model here in case anything needs adjusted. This is starting in the house_evaluator.py file. First this screenshot is of the image names and image variable holders being initialized. If you gather images from sources other than what is labeled below you will need to add a variable everyplace these show up. This shouldn’t be too difficult, I only count four locations that you would need to add an image holder variable and a name variable.\n\nYou can skip past all of the house image quality models to what is now about line 220 which is right after the random picker operation. Lets say your new model identifies window quality, if no image remains then you need to add your variable to the list of variables returned here with windows as None and window confidence as 0\n\nNext scroll past all of the next models (vegetation, siding, gutter, and roof) I left a spot for your window model if this isn’t a hypothetical situation :). Whether your model is binary or has multiple labels changes how you must implement it. For a binary model you can look at the house_present model and the multiple_house model as an example while for a multi option model you can look at any of the attribute models. Below are screenshots of house_present followed by the siding_model, house_present may be misleading but keep in mind it needed to check if an image was present and delete or add so your attribute model will be altered a little from this code and more similair to the start and end of the siding model.\n\n\nFinally add your models variables to the final return statement.\n\nNavigate to city_evaluator.py and update the three spots where it handles variables. This should be simple just copy and paste the lines and change the variable name. An example of the code snippets below: 1. clear_image = attributes.get(“clear_image”)\n\n\n\nif (‘clear_image’ in list(df.columns)):\n__ pass\nelse:\n__ df[‘clear_image’] = None\n\ndf.at[i,‘clear_image’] = clear_image\n\n\n\n\n\n\ncity_evaluator.py already does the work for you, you just need to add links. First double check that all the variables mentioned in the last section is updated or the new values will not be pulled and written. The folder paths you need to change is the path to the parent folder that contains all of the images (below I show Ogden’s links), then you need to make a csv to hold all of your information. I do not think there is a generic database csv file we have but all of the categories can be copied from an existing one. At minimum the csv file must have all of the addresses from the csv file used to create Google urls or nothing will print.\n\nThen update the following lines with your city info and place them with the others in city_evaluator. You can see I just uncomment whichever city I want to run.\nmain_folder = os.path.expanduser(“~/Documents/parent_folder_holder/ogden_address_image”)\ndf = pd.read_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’))\ndf.to_csv(os.path.expanduser(‘~/Documents/GitHub/Housing/Housing Databases/ogden_database.csv’), index = False)\nNow the fun part. If everything worked correctly all you have to do is run city_evaluator and wait for quite a while for all the images to be processed. The terminal will look like below for while it runs through the models.\n\nOgden was smaller so ran a little faster but here is the final result of the current models I have evaluating Ogden:\n\n\n\n\nI think this method goes pretty quick overall. Scraping addresses and cleaning goes quick once you have the functions to clean the data, AI models take a while to understand at first but after one or two the process becomes quicker, and implementing models and printing to CSV files take a while to code the first time but alterations are easy. The worst part of this process is sorting images for models. It can take hours and you may have to do it multiple times to increase the accuracy of models.\nSteps for the rest of this project not included in this guide:\nI will implement a roof model\nI will better train the current models\nBut the best part to go find once it’s done is AI heatmaps which are images that show where in an image a computer is most influenced to make a prediction."
  },
  {
    "objectID": "posts/Gavin-Fisher_week1/Week_One.html",
    "href": "posts/Gavin-Fisher_week1/Week_One.html",
    "title": "Week One Blog",
    "section": "",
    "text": "Datacamp\n\nNine Lessons Completed: Intro to Deep Learning with Keras, Web Scraping in Python, Data Communication Concepts, Image Processing in Python, GitHub Concepts, AI Fundamentals, Introduction to ChatGPT, Introduction to R, Intermediate R. Below are a few examples of what I thought was cool in these lessons. Obviously too much content was covered to display here.\n\nIntro to Deep Learning with Keras\n\n# Import the Sequential model and Dense layer\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an input layer and a hidden layer with 10 neurons\nmodel.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n\n# Add a 1-neuron output layer\nmodel.add(Dense(1))\n\n# Summarise your model\nmodel.summary()\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n dense_2 (Dense)             (None, 10)                30        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 11        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 41\n\n\nTrainable params: 41\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\n\n\n\nNeural Network\n\n\n\n\nImage Proccessing in Python\n\n\n\n\n\nRocket Image\n\n\n# Import the modules from skimage\nfrom skimage import data, color\n\n# Load the rocket image\nrocket = data.rocket()\n\n# Convert the image to grayscale\ngray_scaled_rocket = color.rgb2gray(rocket)\n\n# Show the original image\nshow_image(rocket, 'Original RGB image')\n\n# Show the grayscale image\nshow_image(gray_scaled_rocket, 'Grayscale image')\n\n\n\n\n\nBlack and White Rocket\n\n\n\n\n\n\n\nGrapefruit\n\n\n# Import the canny edge detector \nfrom skimage.feature import canny\n\n# Convert image to grayscale\ngrapefruit = color.rgb2gray(grapefruit)\n\n# Apply canny edge detector\ncanny_edges = canny(grapefruit)\n\n# Show resulting image\nshow_image(canny_edges, \"Edges with Canny\")\n\n\n\n\n\nGrapefruit Edges\n\n\n\n\n\n\n\nBuilding\n\n\n# Import the corner detector related functions and module\nfrom skimage.feature import corner_harris, corner_peaks\n\n# Convert image from RGB-3 to grayscale\nbuilding_image_gray = color.rgb2gray(building_image)\n\n# Apply the detector  to measure the possible corners\nmeasure_image = corner_harris(building_image_gray)\n\n# Find the peaks of the corners using the Harris detector\ncoords = corner_peaks(corner_harris(building_image_gray), min_distance=20, threshold_rel=0.02)\n\n# Show original and resulting image with corners detected\nshow_image(building_image, \"Original\")\nshow_image_with_corners(building_image, coords)\n\n\n\n\n\nBuilding Corners\n\n\nI left out a lot of the cool images I got to make but I would definitely recommend Datacamp for people trying to learn data related topics with R, Python, and machine learning because I found these lessons very helpful and fun. I’m not sure how much it costs because it is provided through DSPG but I think it would be worth it to buy for a month or something and do as many lessons as possible during the summer or so."
  },
  {
    "objectID": "posts/Gavin-Fisher_week2/Week_Two.html",
    "href": "posts/Gavin-Fisher_week2/Week_Two.html",
    "title": "Week Two Blog",
    "section": "",
    "text": "Tidycensus and this Blog\nWe watched videos learning about the basics of Tidycensus from the developer of the package. It taught basic map creation and general use of the application link to video I made one of the graphics from the video and one from his textbook online (I excluded the images made with the walk through). This blog was also created during the second week. Otherwise we started to look into our project.\nlibrary(tigris)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\nia_pumas <- pumas(state = \"IA\", cb = TRUE, year = 2019)\nia_puma_map <- mapview(ia_pumas)\n\n\n\n\n\nPumas in Iowa\n\n\nDM_IA_tracts <- map_dfr(c(\"IA\"), ~{\n  tracts(.x, cb = TRUE, year = 2021)\n}) %>%\n  st_transform(8528)  \n\nDM_metro <- core_based_statistical_areas(cb = TRUE, year = 2021) %>%\n  filter(str_detect(NAME, \"Des Moines\")) %>%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = DM_IA_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = DM_metro, fill = NA, color = \"red\") + \n  theme_void()\n\n\n\n\n\nOutlined Des Moines metro\n\n\nThere are many more cool graphs to make with this library that I did not touch. Between Kyle Walker’s free book online and his lectures on YouTube there is tons to learn and do with the census data using this library. Hopefully this will be helpful later in our project and I will get to learn more."
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using https://iowaassessors.com/ I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. I was responsible for the new vegetation model and siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example, for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels. The new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\nWe also went to the ITAG conference. I went to Bullwall by Don McCraw, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule. I found the Bullwall talk very interesting which explained generally how cybersecurity works and how their product is an extra protection layer."
  },
  {
    "objectID": "posts/Gavin-Fisher_week6/Week_Six.html",
    "href": "posts/Gavin-Fisher_week6/Week_Six.html",
    "title": "Week Six Blog",
    "section": "",
    "text": "Monday DSPG assessed Grundy Center and New Hampton by taking pictures of houses and tracking characteristics of the houses. Tuesday we had a similar experience assessing Independence. A few details came to mind during these events, first in the Fulcrum app we had the option to evaluate roofs labeled AI Roof.\nAssuming that in future years roofing will be added to the list of characteristics evaluated I thought that when looking at roofs you need not only assess the quality of the roof but also the age. I believe there is a difference between a faded old roof and a new damaged roof. If DSPG goes to cities assessing the houses again it would be wise to sit down as a group and discuss what is considered good, fair, and poor in every single category. While assessing with other DSPG members I realized that people’s opinions of these categories greatly differed.\nWhile talking to residents of these towns I was made aware of other issues besides what we were evaluating such as plumbing and flooding issues. Although these are different than what the Housing team is currently directed towards maybe it could be projects for the future in DSPG.\nDue to the AI Roof category I left a space for the roofing model. If I have extra time I will fill this space with two models, one that evaluates whether the roof is damaged and one that evaluates the age. Additionally I added a spot for an AI window evaluator for a model that can predict whether there is a boarded up or broken window, inspired by Google images I downloaded from addresses considered poor or very poor in Des Moines. An option to look for in future years is an AI model that can look for multiple characteristics such as age and damage for roofs. Another we discussed this week is AI heatmaps which show where the AI is looking in the image to determine it’s guess. I really want to attempt this time permitting.\nNow for reading relief:\nI made this graph real quick just to visualize the counties which we are focusing on. Grundy Center is in Grundy County, New Hampton is in Chickasaw County, Independence is in Buchanan County, and Slater is in Story County.\n\nHere is a closer look at the fulcrum app we used to evaluate houses. The first image shows the different folders for each community, followed by the interface for a singular house evaluation.\n\n\n\nNext for the code I built on the skeleton from last week mostly following the code plan diagram I made 2 weeks ago(?). As of right now house_evaluator.py can read in a folder with images in it (as if it was one house from multiple sources). The program will use the house_present_classifier, clear_image_classifier, and multiple_houses_classifier models. These models evaluate whether each image has a visible house, whether the house is obstructed, and if there are multiple houses visible. Ideally we want to evaluate an image with a house, minimal obstruction, and only one house visible. Each model will remove unwanted images from the list in the hopes of choosing only the best image of the house to evaluate.\nIf no image remains the program returns that a better image is needed of the house. If multiple good pictures of the house remain the program will randomly select one. I hope to implement looking at the date of the images before randomly selecting but we do not have dates of images stored from any source yet.\n\nNext the remaining image will be ran through the vegetation, siding, and gutter models. As I said last week the gutter model is on hold until I get more images of poor gutters and spaces in the code are there for roof and window models for either next year or my free time. All of the models predict pretty poorly still because we need a lot more data on bad house images but they are predicting at a 35-45% confidence rate currently.\nAfter I got all of the models in the program properly I started to work on writing to CSV files. The intention was to print each attribute such as vegetation in it’s respective column based on address. I used the CSV library which turned out to take longer than it seems giving me issues such as wiping the file clean, not writing any values, then writing values to the leftmost row available after address. I finally got the attributes to print to the correct column but printed on every row instead of only one row based on address.\nSadat, our AI fellow, used Pandas and could correctly do the correct task in about 10 lines where it took my method about 40 lines of code. Long story short I am now implementing pandas to manipulate the csv files.\nThis code shows the program checking if the columns exist in the csv and if not adding the column. Then the values of each is wrote to the CSV file. After discussion with Sadat I added the model confidence percentage so that as we upload more accurate models we can see how well they are preforming and also give an average confidence for a final report.\n\n\nIn summary this program so far can take in a folder of images, choose the best one, evaluate it’s vegetation, siding, and gutters, then print these attributes to a csv at the same address as the input folder. This python Script was nearing 400 lines of code due to tests and old code I commented out to save. To run many images through this program I think the easiest path would to make a parent folder full of folders labeled by address and inside of the address folders are the images for each address. I did not want a 400 line for loop to iterate through this parent folder so my goal for this weekend is to make two scripts.\nThe first script will read in the files and iterate through them calling the second script each run to evaluate the images within and return house attributes. The first script will take these attributes and return them to a CSV file to the proper column and address. This will be essentially the same as what I have working now but it will be able to read and document many folders of houses rather than individual houses manually placed at a time. I will leave the current script alone since it works and make two completely new scripts to improve this method.\n\n\nToday was very productive, the entire program now works. city_evaluator.py can read in a parent folder which contains address folders, it then calls house_evaluator.py for each address folder. house_evaluator runs images through the AI models (still largely inaccurate) then returns attributes of the image detected. Having multiple files was accomplished by creating a function within house_evaluator then calling that function in city_evaluator.\n\n\nAfter loading in all the variables in city_eval I wrote to the csv by address. This method largely stayed the same from yesterday I just had to tweak it so that the name being printed to was iterated.\n\n\nIn my actual conclusion I will say this was a very successful week even though it was shortened. I plan to make a blog post just for my weekend project so that I don’t have to remember it next Thursday. I will attempt to make an AI heatmap so we can visually see why the models guess incorrectly (right now they suck because we barely have any data). A lot of people I have talked to think of object detection when I say AI heatmap so I will need to explain in depth why this is different. Also in the image above you can see gutter is FALSE and 0 confidence because I have still neglected making that model. I now have access to data maybe I can get it made next week. We also REALLY need to update the other models with the data we collected in Grundy Center, Independence, and New Hampton."
  },
  {
    "objectID": "posts/Gavin-Fisher_week7/Week_Seven.html",
    "href": "posts/Gavin-Fisher_week7/Week_Seven.html",
    "title": "Week Seven Blog",
    "section": "",
    "text": "First I had to resort Google images into folders that could be processed by my city_evaluator.py file. I made a diagram to show how the images are being stored below.\n\nI made an image sorting algorithm that takes a source such as Google’s images then sort each image based on address and make a folder for each address. The reason each town folder has folders with address names is so that when I pull in winvest images as I do below I can have multiple images for the same address to run through the AI models. The next images just show the parent folder and how the folders are stored same as the above graphic.\n\n\nAn update from last week was that instead of printing to a test file I am now able to update each CSV with information needed. I added a test_failed category to show which image quality model kicked the address back and I added the title of the image used for the evaluation. Also added is the gutter model and a space for the roofing model. The first image below shows what happens when no image is found for an address. The second is a picture of New Hampton being evaluated. The reason it is missing multiple values is because the Google streetview had a lot of missing images for New Hampton. I added a third image which is Independence which is filled out from the later part of the week so it is full and has more attributes.\n\n\n\nI made a full guide of how to recreate the results of this project code wise here. I plan to do more before the summer finishes but this guide covers how to scrape images, clean data, make urls, scrape the Google API for streetview images, make models, implement models, and finally export results of models to a CSV for each address. Our team also made the teaser video for our AI housing project.\nI have also made an algorithm that will rename photos from Winvest so that I can sort it into the parent folder and have more than just Google images to evaluate. A lot of this week has been updating and improving the code from last week so that everything works better. On my todo list includes adding the roof model, retrain existing models to improve accuracy, and try to make heatmaps by the end of the project."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "href": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "title": "Weekend Six Blog",
    "section": "",
    "text": "This weekend I set out to make what I refer to as AI heatmaps which is created when an image classification model predicts a class we also track what pixels in the image helped influence it’s prediction. I tried to use pytorch but after lots of code and lots of error I put a temporary pause and explored other libraries. I ended up using keras and resnet for my class activation mapping python script by following along with a video.\n\nAlthough I think this looks like a heatmap I think that the proper name is a class activation map or CAM. Maybe I will refer to it as AI heatmaps anyway.\nFrom this video I got a majority of the code but some of the libraries were outdated so I had to use the pillow library for images rather than keras images. Resnet also seemed to change the way you could call layers so I had to change the call to the exact name of the layer which could be really inefficient if the amount of layers change. Otherwise the code as of right now is pretty much the same.\n\n\nI want to alter this code a little however. The classifiers and models are not the ones we built, so I think it would be pretty cool if we could implement our housing models into this method to see how well the models are operating with visual examples. Knowing that our models are not very accurate this could help us to know what specific features we need more images of or which features give the model false guesses."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html",
    "href": "posts/Gavin-Fisher_weekend7-8/Weekend_Seven_Eight.html",
    "title": "Weekend Seven Blog",
    "section": "",
    "text": "During this long weekend I learned more about AI models and toyed around with the heat maps I wanted to get done. The SHAP method mostly works and the CAM method prints but does not work quite yet and I will show both of these below.\nFirst I finished a course in DataCamp labeled Deep Learning with PyTorch. This course gave me a lot more insight on how to build layers of Neural Networks and gave some cool graphics I wanted to display after my ramble.\nI learned about some of the math behind machine learning such as matrix multiplication, derivatives, and how weights are set to each connection of the network.\n\n\n\n\nNext I made an AI model that prints SHAP images from it’s prediction. I used the gutter model first then made another for house present. Currently the image is fed to the AI too blurry for it to see the gutter so I need to go back and give the AI a more detailed image to evaluate but in the meantime I made another model for the house present data.\nThese models use keras and sklearn to build the network just as our previous models and then uses the shap library to create images that show basically what the AI is using to make it’s prediction. These models were also made in Google Colab.\n\n\n\n\nAs you can see I tried to display the data a few different way’s and they definitely need to be tweaked. The last three images are the SHAP images, my favorite is the last one. It is showing all of those empty graphs for some reason but the actual images shown is what we want. It shows which pixels the model is looking at to make each determination. You may notice the two images are basically the same but this makes sense because if the model determines one line of pixels shows a house is probably present the model will use the same pixels to determine that the no house present category is false.\nFinally I tried to apply what I learned about PyTorch to make a CAM model. I made this model in a Py file just to see the difference of running in Google Colab. This model uses torch, torchvision, and sklearn.\n\n\nI got the image to display but as you can see for an example of a house present and no house present the heat map is exactly the same which probably means it is not tracking the neural network properly.\nI think this all is a good step forward and hopefully was enough to finish this by our presentation next week.\nAlso it’s Independence Day, God Bless the USA"
  },
  {
    "objectID": "posts/House_blog/House_blog.html",
    "href": "posts/House_blog/House_blog.html",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#happies",
    "href": "posts/House_blog/House_blog.html#happies",
    "title": "Housing Team Recap Week Two",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#crappies",
    "href": "posts/House_blog/House_blog.html#crappies",
    "title": "Housing Team Recap Week Two",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "href": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Recap Week Two",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "href": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "title": "Housing Team Recap Week Two",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nAngelina\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "href": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "title": "Housing Team Recap Week Two",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "posts/House_blog/House_blog.html#questions-discussion",
    "href": "posts/House_blog/House_blog.html#questions-discussion",
    "title": "Housing Team Recap Week Two",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html",
    "href": "posts/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  }
]