[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog’s purpose is to keep track of code done during Gavin D Fisher’s time at Iowa State’s Data Science for the Public Good program.\n\nLinks to other members of the DSPG AI Housing Team\nUndergrads: Angelina, Kailyn\nTeam leads: Morenike, Sadat\nDSPG Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gavin’s DSPG Blog",
    "section": "",
    "text": "Week Five Blog\n\n\n\n\n\n\n\nWeek Five\n\n\nAI Models\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nHousing Team Recap Week Five\n\n\n\n\n\n\n\nWeek Five\n\n\nHousing Team\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four Blog\n\n\n\n\n\n\n\nWeek Four\n\n\nWeb Scraping\n\n\nProject Plan\n\n\nExcel\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Blog\n\n\n\n\n\n\n\nWeek Three\n\n\nAI Models\n\n\nProject Plan\n\n\nJupyter Notebook\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nHousing Team Recap Week Two\n\n\n\n\n\n\n\nWeek Two\n\n\nHousing Team\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two Blog\n\n\n\n\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek One Blog\n\n\n\n\n\n\n\nWeek One\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gavin-Fisher_week1/Week_One.html",
    "href": "posts/Gavin-Fisher_week1/Week_One.html",
    "title": "Week One Blog",
    "section": "",
    "text": "Datacamp\n\nNine Lessons Completed: Intro to Deep Learning with Keras, Web Scraping in Python, Data Communication Concepts, Image Processing in Python, GitHub Concepts, AI Fundamentals, Introduction to ChatGPT, Introduction to R, Intermediate R. Below are a few examples of what I thought was cool in these lessons. Obviously too much content was covered to display here.\n\nIntro to Deep Learning with Keras\n# Import the Sequential model and Dense layer\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an input layer and a hidden layer with 10 neurons\nmodel.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n\n# Add a 1-neuron output layer\nmodel.add(Dense(1))\n\n# Summarise your model\nmodel.summary()\n\n\n\n\n\nNeural Network\n\n\n\n\nImage Proccessing in Python\n\n\n\n\n\nRocket Image\n\n\n# Import the modules from skimage\nfrom skimage import data, color\n\n# Load the rocket image\nrocket = data.rocket()\n\n# Convert the image to grayscale\ngray_scaled_rocket = color.rgb2gray(rocket)\n\n# Show the original image\nshow_image(rocket, 'Original RGB image')\n\n# Show the grayscale image\nshow_image(gray_scaled_rocket, 'Grayscale image')\n\n\n\n\n\nBlack and White Rocket\n\n\n\n\n\n\n\nGrapefruit\n\n\n# Import the canny edge detector \nfrom skimage.feature import canny\n\n# Convert image to grayscale\ngrapefruit = color.rgb2gray(grapefruit)\n\n# Apply canny edge detector\ncanny_edges = canny(grapefruit)\n\n# Show resulting image\nshow_image(canny_edges, \"Edges with Canny\")\n\n\n\n\n\nGrapefruit Edges\n\n\n\n\n\n\n\nBuilding\n\n\n# Import the corner detector related functions and module\nfrom skimage.feature import corner_harris, corner_peaks\n\n# Convert image from RGB-3 to grayscale\nbuilding_image_gray = color.rgb2gray(building_image)\n\n# Apply the detector  to measure the possible corners\nmeasure_image = corner_harris(building_image_gray)\n\n# Find the peaks of the corners using the Harris detector\ncoords = corner_peaks(corner_harris(building_image_gray), min_distance=20, threshold_rel=0.02)\n\n# Show original and resulting image with corners detected\nshow_image(building_image, \"Original\")\nshow_image_with_corners(building_image, coords)\n\n\n\n\n\nBuilding Corners"
  },
  {
    "objectID": "posts/Gavin-Fisher_week2/Week_Two.html",
    "href": "posts/Gavin-Fisher_week2/Week_Two.html",
    "title": "Week Two Blog",
    "section": "",
    "text": "Tidycensus and this Blog\nWe watched videos learning about the basics of Tidycensus from the developer of the package. It taught basic map creation and general use of the application link to video I made one of the graphics from the video and one from his textbook online (I excluded the images made with the walk through). This blog was also created during the second week. Otherwise we started to look into our project.\nlibrary(tigris)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\nia_pumas <- pumas(state = \"IA\", cb = TRUE, year = 2019)\nia_puma_map <- mapview(ia_pumas)\n\n\n\n\n\nPumas in Iowa\n\n\nDM_IA_tracts <- map_dfr(c(\"IA\"), ~{\n  tracts(.x, cb = TRUE, year = 2021)\n}) %>%\n  st_transform(8528)  \n\nDM_metro <- core_based_statistical_areas(cb = TRUE, year = 2021) %>%\n  filter(str_detect(NAME, \"Des Moines\")) %>%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = DM_IA_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = DM_metro, fill = NA, color = \"red\") + \n  theme_void()\n\n\n\n\n\nOutlined Des Moines metro"
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using https://iowaassessors.com/ I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week."
  },
  {
    "objectID": "posts/House_blog/House_blog.html",
    "href": "posts/House_blog/House_blog.html",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#happies",
    "href": "posts/House_blog/House_blog.html#happies",
    "title": "Housing Team Recap Week Two",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#crappies",
    "href": "posts/House_blog/House_blog.html#crappies",
    "title": "Housing Team Recap Week Two",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "href": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Recap Week Two",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "href": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "title": "Housing Team Recap Week Two",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nAngelina\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "href": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "title": "Housing Team Recap Week Two",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "posts/House_blog/House_blog.html#questions-discussion",
    "href": "posts/House_blog/House_blog.html#questions-discussion",
    "title": "Housing Team Recap Week Two",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html",
    "href": "posts/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT."
  }
]