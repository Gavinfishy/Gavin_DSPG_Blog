[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Gavin is from Bolingbrook Illinois, an upcoming Junior at Iowa State University with a major in Computer Science and a minor in Data Science. He is part of the Cardinal Space Mining Club as part of the controls team and soon to be treasurer of the club. This blog’s purpose is to track Gavin’s work done at DSPG as part of the AI housing team.\n\nLinks to other members of the DSPG AI Housing Team\nUndergrads: Angelina, Kailyn\nTeam leads: Morenike, Sadat\nDSPG Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gavin’s DSPG Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGuide: How to Evaluate a City\n\n\n8 min\n\n\n\nWeek Seven\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\nR\n\n\nGuide\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeekend Six Blog\n\n\n1 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nWeekend\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Six Blog\n\n\n6 min\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\nExcel\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Five Blog\n\n\n2 min\n\n\n\nWeek Five\n\n\nAI Models\n\n\nPython\n\n\nGoogle Colab\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Five\n\n\n3 min\n\n\n\nWeek Five\n\n\nHousing Team\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Four Blog\n\n\n3 min\n\n\n\nWeek Four\n\n\nWeb Scraping\n\n\nProject Plan\n\n\nExcel\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Three Blog\n\n\n1 min\n\n\n\nWeek Three\n\n\nAI Models\n\n\nProject Plan\n\n\nJupyter Notebook\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nJune 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Team Recap Week Two\n\n\n0 min\n\n\n\nWeek Two\n\n\nHousing Team\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Two Blog\n\n\n1 min\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek One Blog\n\n\n1 min\n\n\n\nWeek One\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\nGavin D. Fisher\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "href": "posts/Gavin-Fisher_guide_w7/full_guide.html",
    "title": "Guide: How to Evaluate a City",
    "section": "",
    "text": "This will be a full guide on how to recreate what AI housing team has done this summer using Ogden IA as an example.\n\n\nFirst we need to go to Beacon to try and collect a list of addresses. The following link will bring you to Boone County and you need to scroll in on Ogden (West/center). Beacon Map View\nWe found a tool called Instant Data Scraper that you need for this process. An alternative to this is badly needed as you will see soon, but this application worked for the time being when we had no prior web scraping experience yet. DSPG students for summer of 2024, I beg you to try and scrape Beacon with your own spider before using this tool to save yourself from the cleaning you have to do.\nBelow I display an image of the Beacon interface then an image zoomed in on Ogden.\n\n\nNavigate to the buttons towards the top of the screen and find the cursor over a box. This is the select tool that we need to use for selecting properties. There is a limit of how many addresses you can grab at once so below is what it looks like if you try to grab every property at once:\n\nInstead we need to click on this button and select the polygon version.\n\nTo avoid the problem of too many addresses I typically select four segments of the town and merge them after exporting to csv files. As you can see below I used the polygon tool to select all parcels north of 216th Street(the large road below my box). The polygon selector has multiple points you can set, just make sure each parcel you want is within the shaded area or on the line then double click to select all parcels.\n\nNotice on the right hand side how all of the parcels you selected show up. This is where we want to scrape the information from. Open the instant data scraper tool from your extensions button (top right). Below you can see how the chrome extension automatically found the information to the right which now has a red marker around it. I deleted all of the columns in this interface except for resultitem because from our experience this holds the parcel number, address, and owner.\n\n\nGo ahead and export to a csv. The file is automatically saved as beacon.csv so go ahead and rename this file to ogden_n_216_st.csv. This is a temporary name just so you can easily find your Ogden files and know that the first grab was north of this road that we chose. Repeat this process until you have grabbed all of the parcels you desire, you have to close the instant data scraper and open it again for each new grab. Some towns I could scrape in one go while some took four cuts to collect all the data.\nNext we are simply merging the csv files. Choose one of the files to be the main one, then go csv by csv to copy and paste into the main csv file. You could make a script to do this for you but honestly it doesn’t take that long, a trick is when you press ctrl + shift + down arrow you can select the entire column. ctrl + down arrow will bring you to the bottom of the csv column in your main file. Leave out the top row that has resultitem. Also all of the boxes have - or #NAME? don’t worry yet there is more in these boxes they are just on newlines.\nRename the main file to ogden_addresses because this in this file we will clean the data then create address links. First in B2 place this function =TRIM(CLEAN(SUBSTITUTE(A2,CHAR(160),” “))). This will reformat the input text to be on one line without hidden characters. Usually your able to double click the bottom right of the cell and it will auto fill all the way down but most of these functions refuse to auto fill so you just need to grab the corner and drag to fill in B to be easily accessible text. Next copy all of the cells in column B and paste as text into column C.\nCopy column C into column D. Select all of column D, navigate to the Data tab, then text to columns in data tools. Select delimited then press next, deselect tab then select other and enter - into the box then select next, then finish. This will separate the data by parcel number, address, and owner. I deleted the owner name due to it being unnecessary information.\n\nFinally I noticed address needs to be trimmed so in a new column you can use the function =TRIM(A2) then paste the result as values back into the address column.\n\n\n\nWhat are the Google API Links? Here is an example of the beginning of an address from Grundy Center:\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=303+I+AVE,+GRUNDY+CENTER+IOWA\nthis is followed by &key=(API Key). Pasting the entire link brings you to an image provided by Google street view if it exists. You could also use latitude and longitude coordinates instead of address but the image you get is not guaranteed to be a front image of the house. Google however, has a built in program to get the front of a house if you enter the address if it can find one.\nWe will continue to use the file from the previous section. First you need a url_start column to store the first half of the url which is always the same (https://maps.googleapis.com/maps/api/streetview?size=800x800&location=). So in columns G, H, I, I have the url in the previous sentence, City (Ogden), then State (IOWA). Next we need to concatenate the full address with =CONCAT(F2, “,”, H2, ” “, I2). F2 is the house address, H2 is city, and I2 is State which results in 119 W SYCAMORE ST, OGDEN IOWA for my first address. Copy this column into the next and paste as values. Do not forget to try double clicking the bottom right of the cell to auto fill before dragging.\nIn the next column use =SUBSTITUTE(K2, ” “,”+“) which will replace all the white space with + which is neccesary for the link. Again paste as values into the next column. Next =CONCAT(G2,M2) will combine what is my url_start column and my full_address_+ column to get the entire url needed to run through my Google API scraper.Finally we need to place the values of this last column I named url_full into the first column so it is easily accessible by the python script.\n\nNow you can grab one of the links from the leftmost column and check that the link works. All I did was copy that part of the link into my browser, added &key=(API Key) and this was my result:\n\n\n\n\nFor this section you need to obtain a Google API key to scrape images from the Google street view API. You also need to download R studio or an IDE that can run R code. If you have access to the DSPG Housing repository there is a folder named complete links which has a grab_images.R file. Below is the code for grabbing Google images for Ogden, keep in mind downloading images takes a very long time many of the files of 3k photos have taken me upwards of an hour to download. I also do not know how to control where the image file is uploaded. My current directory is my blog and the images are uploaded there, I assume there is a way to change the directory in R studio.\nSome errors I ran into while scraping: Google does not like addresses that are combined such as 123/456 Main St and it will cause an error. Some addresses you pull will start with a # and Google will not accept this. Lines that have the #NAME? error I delete the row. Some addresses I pulled from Ogden did not have an address but just an owner name which caused my program to throw an error. If the address is empty or does not start with a number I delete the row. I manually deleted just over 100 rows from the Ogden set which had missing addresses and was blank or filled in with owner name. This is a good example of why a personalized scraper should be made because while the chrome extension is convenient and fast it pulls back many issues with the data which we may get by scraping ourselves.\n(insert grab images ogden code)\nSome common errors with the images coming in are as follows: If Google does not have an image it will give you an image does not exist image but this should be easily identified by our AI model through training. Quick note if you retrain the models I show later in this guide only put 2-3 of these error images as to not throw off the model, having 50 or so will make the model think that image has a very high correlation. Next I have had an odd issue where the same image will be used for multiple addresses. This is quite weird because I do not know where the image is getting pulled from. Maybe there is a default image that is getting pulled somehow? But it is not just one image for Grundy Center which has about 3,500 images we pulled I saw anywhere from 10-20% of the images were duplicates with different addresses. There were probably 8-10 different images that were duplicated for different images so this is a big problem that doesn’t always happen but is a mystery to me why this happens. But hey 80-90% good images is a passing grade."
  },
  {
    "objectID": "posts/Gavin-Fisher_week1/Week_One.html",
    "href": "posts/Gavin-Fisher_week1/Week_One.html",
    "title": "Week One Blog",
    "section": "",
    "text": "Datacamp\n\nNine Lessons Completed: Intro to Deep Learning with Keras, Web Scraping in Python, Data Communication Concepts, Image Processing in Python, GitHub Concepts, AI Fundamentals, Introduction to ChatGPT, Introduction to R, Intermediate R. Below are a few examples of what I thought was cool in these lessons. Obviously too much content was covered to display here.\n\nIntro to Deep Learning with Keras\n\n# Import the Sequential model and Dense layer\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an input layer and a hidden layer with 10 neurons\nmodel.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n\n# Add a 1-neuron output layer\nmodel.add(Dense(1))\n\n# Summarise your model\nmodel.summary()\n\nModel: \"sequential_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n dense_2 (Dense)             (None, 10)                30        \n\n\n                                                                 \n\n\n dense_3 (Dense)             (None, 1)                 11        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 41\n\n\nTrainable params: 41\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\n\n\n\n\nNeural Network\n\n\n\n\nImage Proccessing in Python\n\n\n\n\n\nRocket Image\n\n\n# Import the modules from skimage\nfrom skimage import data, color\n\n# Load the rocket image\nrocket = data.rocket()\n\n# Convert the image to grayscale\ngray_scaled_rocket = color.rgb2gray(rocket)\n\n# Show the original image\nshow_image(rocket, 'Original RGB image')\n\n# Show the grayscale image\nshow_image(gray_scaled_rocket, 'Grayscale image')\n\n\n\n\n\nBlack and White Rocket\n\n\n\n\n\n\n\nGrapefruit\n\n\n# Import the canny edge detector \nfrom skimage.feature import canny\n\n# Convert image to grayscale\ngrapefruit = color.rgb2gray(grapefruit)\n\n# Apply canny edge detector\ncanny_edges = canny(grapefruit)\n\n# Show resulting image\nshow_image(canny_edges, \"Edges with Canny\")\n\n\n\n\n\nGrapefruit Edges\n\n\n\n\n\n\n\nBuilding\n\n\n# Import the corner detector related functions and module\nfrom skimage.feature import corner_harris, corner_peaks\n\n# Convert image from RGB-3 to grayscale\nbuilding_image_gray = color.rgb2gray(building_image)\n\n# Apply the detector  to measure the possible corners\nmeasure_image = corner_harris(building_image_gray)\n\n# Find the peaks of the corners using the Harris detector\ncoords = corner_peaks(corner_harris(building_image_gray), min_distance=20, threshold_rel=0.02)\n\n# Show original and resulting image with corners detected\nshow_image(building_image, \"Original\")\nshow_image_with_corners(building_image, coords)\n\n\n\n\n\nBuilding Corners\n\n\nI left out a lot of the cool images I got to make but I would definitely recommend Datacamp for people trying to learn data related topics with R, Python, and machine learning because I found these lessons very helpful and fun. I’m not sure how much it costs because it is provided through DSPG but I think it would be worth it to buy for a month or something and do as many lessons as possible during the summer or so."
  },
  {
    "objectID": "posts/Gavin-Fisher_week2/Week_Two.html",
    "href": "posts/Gavin-Fisher_week2/Week_Two.html",
    "title": "Week Two Blog",
    "section": "",
    "text": "Tidycensus and this Blog\nWe watched videos learning about the basics of Tidycensus from the developer of the package. It taught basic map creation and general use of the application link to video I made one of the graphics from the video and one from his textbook online (I excluded the images made with the walk through). This blog was also created during the second week. Otherwise we started to look into our project.\nlibrary(tigris)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\nia_pumas <- pumas(state = \"IA\", cb = TRUE, year = 2019)\nia_puma_map <- mapview(ia_pumas)\n\n\n\n\n\nPumas in Iowa\n\n\nDM_IA_tracts <- map_dfr(c(\"IA\"), ~{\n  tracts(.x, cb = TRUE, year = 2021)\n}) %>%\n  st_transform(8528)  \n\nDM_metro <- core_based_statistical_areas(cb = TRUE, year = 2021) %>%\n  filter(str_detect(NAME, \"Des Moines\")) %>%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = DM_IA_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = DM_metro, fill = NA, color = \"red\") + \n  theme_void()\n\n\n\n\n\nOutlined Des Moines metro\n\n\nThere are many more cool graphs to make with this library that I did not touch. Between Kyle Walker’s free book online and his lectures on YouTube there is tons to learn and do with the census data using this library. Hopefully this will be helpful later in our project and I will get to learn more."
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using https://iowaassessors.com/ I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. I was responsible for the new vegetation model and siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example, for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels. The new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\nWe also went to the ITAG conference. I went to Bullwall by Don McCraw, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule. I found the Bullwall talk very interesting which explained generally how cybersecurity works and how their product is an extra protection layer."
  },
  {
    "objectID": "posts/Gavin-Fisher_week6/Week_Six.html",
    "href": "posts/Gavin-Fisher_week6/Week_Six.html",
    "title": "Week Six Blog",
    "section": "",
    "text": "Monday DSPG assessed Grundy Center and New Hampton by taking pictures of houses and tracking characteristics of the houses. Tuesday we had a similar experience assessing Independence. A few details came to mind during these events, first in the Fulcrum app we had the option to evaluate roofs labeled AI Roof.\nAssuming that in future years roofing will be added to the list of characteristics evaluated I thought that when looking at roofs you need not only assess the quality of the roof but also the age. I believe there is a difference between a faded old roof and a new damaged roof. If DSPG goes to cities assessing the houses again it would be wise to sit down as a group and discuss what is considered good, fair, and poor in every single category. While assessing with other DSPG members I realized that people’s opinions of these categories greatly differed.\nWhile talking to residents of these towns I was made aware of other issues besides what we were evaluating such as plumbing and flooding issues. Although these are different than what the Housing team is currently directed towards maybe it could be projects for the future in DSPG.\nDue to the AI Roof category I left a space for the roofing model. If I have extra time I will fill this space with two models, one that evaluates whether the roof is damaged and one that evaluates the age. Additionally I added a spot for an AI window evaluator for a model that can predict whether there is a boarded up or broken window, inspired by Google images I downloaded from addresses considered poor or very poor in Des Moines. An option to look for in future years is an AI model that can look for multiple characteristics such as age and damage for roofs. Another we discussed this week is AI heatmaps which show where the AI is looking in the image to determine it’s guess. I really want to attempt this time permitting.\nNow for reading relief:\nI made this graph real quick just to visualize the counties which we are focusing on. Grundy Center is in Grundy County, New Hampton is in Chickasaw County, Independence is in Buchanan County, and Slater is in Story County.\n\nHere is a closer look at the fulcrum app we used to evaluate houses. The first image shows the different folders for each community, followed by the interface for a singular house evaluation.\n\n\n\nNext for the code I built on the skeleton from last week mostly following the code plan diagram I made 2 weeks ago(?). As of right now house_evaluator.py can read in a folder with images in it (as if it was one house from multiple sources). The program will use the house_present_classifier, clear_image_classifier, and multiple_houses_classifier models. These models evaluate whether each image has a visible house, whether the house is obstructed, and if there are multiple houses visible. Ideally we want to evaluate an image with a house, minimal obstruction, and only one house visible. Each model will remove unwanted images from the list in the hopes of choosing only the best image of the house to evaluate.\nIf no image remains the program returns that a better image is needed of the house. If multiple good pictures of the house remain the program will randomly select one. I hope to implement looking at the date of the images before randomly selecting but we do not have dates of images stored from any source yet.\n\nNext the remaining image will be ran through the vegetation, siding, and gutter models. As I said last week the gutter model is on hold until I get more images of poor gutters and spaces in the code are there for roof and window models for either next year or my free time. All of the models predict pretty poorly still because we need a lot more data on bad house images but they are predicting at a 35-45% confidence rate currently.\nAfter I got all of the models in the program properly I started to work on writing to CSV files. The intention was to print each attribute such as vegetation in it’s respective column based on address. I used the CSV library which turned out to take longer than it seems giving me issues such as wiping the file clean, not writing any values, then writing values to the leftmost row available after address. I finally got the attributes to print to the correct column but printed on every row instead of only one row based on address.\nSadat, our AI fellow, used Pandas and could correctly do the correct task in about 10 lines where it took my method about 40 lines of code. Long story short I am now implementing pandas to manipulate the csv files.\nThis code shows the program checking if the columns exist in the csv and if not adding the column. Then the values of each is wrote to the CSV file. After discussion with Sadat I added the model confidence percentage so that as we upload more accurate models we can see how well they are preforming and also give an average confidence for a final report.\n\n\nIn summary this program so far can take in a folder of images, choose the best one, evaluate it’s vegetation, siding, and gutters, then print these attributes to a csv at the same address as the input folder. This python Script was nearing 400 lines of code due to tests and old code I commented out to save. To run many images through this program I think the easiest path would to make a parent folder full of folders labeled by address and inside of the address folders are the images for each address. I did not want a 400 line for loop to iterate through this parent folder so my goal for this weekend is to make two scripts.\nThe first script will read in the files and iterate through them calling the second script each run to evaluate the images within and return house attributes. The first script will take these attributes and return them to a CSV file to the proper column and address. This will be essentially the same as what I have working now but it will be able to read and document many folders of houses rather than individual houses manually placed at a time. I will leave the current script alone since it works and make two completely new scripts to improve this method.\n\n\nToday was very productive, the entire program now works. city_evaluator.py can read in a parent folder which contains address folders, it then calls house_evaluator.py for each address folder. house_evaluator runs images through the AI models (still largely inaccurate) then returns attributes of the image detected. Having multiple files was accomplished by creating a function within house_evaluator then calling that function in city_evaluator.\n\n\nAfter loading in all the variables in city_eval I wrote to the csv by address. This method largely stayed the same from yesterday I just had to tweak it so that the name being printed to was iterated.\n\n\nIn my actual conclusion I will say this was a very successful week even though it was shortened. I plan to make a blog post just for my weekend project so that I don’t have to remember it next Thursday. I will attempt to make an AI heatmap so we can visually see why the models guess incorrectly (right now they suck because we barely have any data). A lot of people I have talked to think of object detection when I say AI heatmap so I will need to explain in depth why this is different. Also in the image above you can see gutter is FALSE and 0 confidence because I have still neglected making that model. I now have access to data maybe I can get it made next week. We also REALLY need to update the other models with the data we collected in Grundy Center, Independence, and New Hampton."
  },
  {
    "objectID": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "href": "posts/Gavin-Fisher_weekend6-7/Weekend_Six_Seven.html",
    "title": "Weekend Six Blog",
    "section": "",
    "text": "This weekend I set out to make what I refer to as AI heatmaps which is created when an image classification model predicts a class we also track what pixels in the image helped influence it’s prediction. I tried to use pytorch but after lots of code and lots of error I put a temporary pause and explored other libraries. I ended up using keras and resnet for my class activation mapping python script by following along with a video.\n\nAlthough I think this looks like a heatmap I think that the proper name is a class activation map or CAM. Maybe I will refer to it as AI heatmaps anyway.\nFrom this video I got a majority of the code but some of the libraries were outdated so I had to use the pillow library for images rather than keras images. Resnet also seemed to change the way you could call layers so I had to change the call to the exact name of the layer which could be really inefficient if the amount of layers change. Otherwise the code as of right now is pretty much the same.\n\n\nI want to alter this code a little however. The classifiers and models are not the ones we built, so I think it would be pretty cool if we could implement our housing models into this method to see how well the models are operating with visual examples. Knowing that our models are not very accurate this could help us to know what specific features we need more images of or which features give the model false guesses."
  },
  {
    "objectID": "posts/House_blog/House_blog.html",
    "href": "posts/House_blog/House_blog.html",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#happies",
    "href": "posts/House_blog/House_blog.html#happies",
    "title": "Housing Team Recap Week Two",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#crappies",
    "href": "posts/House_blog/House_blog.html#crappies",
    "title": "Housing Team Recap Week Two",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "href": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Recap Week Two",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "href": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "title": "Housing Team Recap Week Two",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nAngelina\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "href": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "title": "Housing Team Recap Week Two",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "posts/House_blog/House_blog.html#questions-discussion",
    "href": "posts/House_blog/House_blog.html#questions-discussion",
    "title": "Housing Team Recap Week Two",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html",
    "href": "posts/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  }
]