[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Gavin is from Bolingbrook Illinois, an upcoming Junior at Iowa State University with a major in Computer Science and a minor in Data Science. He is part of the Cardinal Space Mining Club as part of the controls team and soon to be treasurer of the club. This blog’s purpose is to track Gavin’s work done at DSPG as part of the AI housing team.\n\nLinks to other members of the DSPG AI Housing Team\nUndergrads: Angelina, Kailyn\nTeam leads: Morenike, Sadat\nDSPG Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gavin’s DSPG Blog",
    "section": "",
    "text": "Week Six Blog\n\n\n\n\n\n\n\nWeek Six\n\n\nAI Models\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five Blog\n\n\n\n\n\n\n\nWeek Five\n\n\nAI Models\n\n\nPython\n\n\nGoogle Colab\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nHousing Team Recap Week Five\n\n\n\n\n\n\n\nWeek Five\n\n\nHousing Team\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four Blog\n\n\n\n\n\n\n\nWeek Four\n\n\nWeb Scraping\n\n\nProject Plan\n\n\nExcel\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three Blog\n\n\n\n\n\n\n\nWeek Three\n\n\nAI Models\n\n\nProject Plan\n\n\nJupyter Notebook\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nHousing Team Recap Week Two\n\n\n\n\n\n\n\nWeek Two\n\n\nHousing Team\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two Blog\n\n\n\n\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\n  \n\n\n\n\nWeek One Blog\n\n\n\n\n\n\n\nWeek One\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nGavin Fisher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Gavin-Fisher_week1/Week_One.html",
    "href": "posts/Gavin-Fisher_week1/Week_One.html",
    "title": "Week One Blog",
    "section": "",
    "text": "Datacamp\n\nNine Lessons Completed: Intro to Deep Learning with Keras, Web Scraping in Python, Data Communication Concepts, Image Processing in Python, GitHub Concepts, AI Fundamentals, Introduction to ChatGPT, Introduction to R, Intermediate R. Below are a few examples of what I thought was cool in these lessons. Obviously too much content was covered to display here.\n\nIntro to Deep Learning with Keras\n# Import the Sequential model and Dense layer\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add an input layer and a hidden layer with 10 neurons\nmodel.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n\n# Add a 1-neuron output layer\nmodel.add(Dense(1))\n\n# Summarise your model\nmodel.summary()\n\n\n\n\n\nNeural Network\n\n\n\n\nImage Proccessing in Python\n\n\n\n\n\nRocket Image\n\n\n# Import the modules from skimage\nfrom skimage import data, color\n\n# Load the rocket image\nrocket = data.rocket()\n\n# Convert the image to grayscale\ngray_scaled_rocket = color.rgb2gray(rocket)\n\n# Show the original image\nshow_image(rocket, 'Original RGB image')\n\n# Show the grayscale image\nshow_image(gray_scaled_rocket, 'Grayscale image')\n\n\n\n\n\nBlack and White Rocket\n\n\n\n\n\n\n\nGrapefruit\n\n\n# Import the canny edge detector \nfrom skimage.feature import canny\n\n# Convert image to grayscale\ngrapefruit = color.rgb2gray(grapefruit)\n\n# Apply canny edge detector\ncanny_edges = canny(grapefruit)\n\n# Show resulting image\nshow_image(canny_edges, \"Edges with Canny\")\n\n\n\n\n\nGrapefruit Edges\n\n\n\n\n\n\n\nBuilding\n\n\n# Import the corner detector related functions and module\nfrom skimage.feature import corner_harris, corner_peaks\n\n# Convert image from RGB-3 to grayscale\nbuilding_image_gray = color.rgb2gray(building_image)\n\n# Apply the detector  to measure the possible corners\nmeasure_image = corner_harris(building_image_gray)\n\n# Find the peaks of the corners using the Harris detector\ncoords = corner_peaks(corner_harris(building_image_gray), min_distance=20, threshold_rel=0.02)\n\n# Show original and resulting image with corners detected\nshow_image(building_image, \"Original\")\nshow_image_with_corners(building_image, coords)\n\n\n\n\n\nBuilding Corners"
  },
  {
    "objectID": "posts/Gavin-Fisher_week2/Week_Two.html",
    "href": "posts/Gavin-Fisher_week2/Week_Two.html",
    "title": "Week Two Blog",
    "section": "",
    "text": "Tidycensus and this Blog\nWe watched videos learning about the basics of Tidycensus from the developer of the package. It taught basic map creation and general use of the application link to video I made one of the graphics from the video and one from his textbook online (I excluded the images made with the walk through). This blog was also created during the second week. Otherwise we started to look into our project.\nlibrary(tigris)\nlibrary(mapview)\noptions(tigris_use_cache = TRUE)\nia_pumas <- pumas(state = \"IA\", cb = TRUE, year = 2019)\nia_puma_map <- mapview(ia_pumas)\n\n\n\n\n\nPumas in Iowa\n\n\nDM_IA_tracts <- map_dfr(c(\"IA\"), ~{\n  tracts(.x, cb = TRUE, year = 2021)\n}) %>%\n  st_transform(8528)  \n\nDM_metro <- core_based_statistical_areas(cb = TRUE, year = 2021) %>%\n  filter(str_detect(NAME, \"Des Moines\")) %>%\n  st_transform(8528)\n\nggplot() + \n  geom_sf(data = DM_IA_tracts, fill = \"white\", color = \"grey\") + \n  geom_sf(data = DM_metro, fill = NA, color = \"red\") + \n  theme_void()\n\n\n\n\n\nOutlined Des Moines metro"
  },
  {
    "objectID": "posts/Gavin-Fisher_week3/Week_Three.html",
    "href": "posts/Gavin-Fisher_week3/Week_Three.html",
    "title": "Week Three Blog",
    "section": "",
    "text": "During this week a base AI model was made that can evaluate whether houses have vegetation in front of the property or not.This model evaluates two inputs of image folders or labels using just under 400 images in each category. This binary model can be reused later on the AI housing project and expanded to accomplish more complex tasks.The images used were downloaded from a kaggle(website) data set which has approximately 20,000 images of houses. I estimate that 15-20% of the data set includes pictures of boats, maps, extremely expensive houses, and birds eye view images. To gather our images for the AI model we had to manually go through and sort which houses have vegetation in front of them and which do not excluding images of bad images listed above, confusing images, houses in deserts, houses with snow, and houses in forests due to all of these are different than what we will see in Midwestern houses. In total we had about 750 images about 350 for non vegetation and 400 vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext a rough draft of our project plan was created using visio. This was created to make a neat and more visual representation of the AI housing project path.\n\n\n\n\n\nHousing AI Project Plan\n\n\nFinally, we looked further into what can be scraped off of Vanguard, Beacon, Trulia, and from the Google Maps API.Using https://iowaassessors.com/ I looked at Story counter for the city of Slater. Roads appear to be counted as parcels which is challenging while trying to collect addresses data."
  },
  {
    "objectID": "posts/Gavin-Fisher_week4/Week_Four.html",
    "href": "posts/Gavin-Fisher_week4/Week_Four.html",
    "title": "Week Four Blog",
    "section": "",
    "text": "This week the housing team focused pretty heavily on figuring out how to web scrape. The beginning of the week consisted of collecting addresses from Beacon and Vanguard by using the parcel map selection features and scraping the data. I found a chrome extension simply named instant data scraper but unfortunately you cannot change the data to scrape very easily or alter how the data is inputted. An issue with Beacon is there is a limit of a thousand items to select so I cut Grundy Center and New Hampton into four quadrents divided by roads I picked at about the quarter mark.\n\nWith this tool Grundy Center, New Hampton, and Slater were all easily downloadable into csv files (Grundy Center and New Hampton had to be sliced into for CSV’s then merged due to limits of beacons select feature). It was somewhat a pain cleaning this data we collected, because parcel number, address, and other info were placed into a single excel box with hidden newline characters. We used the following excel functions to parse through the data and collect addresses and form the Google API links with them.\n=TRIM(CLEAN(SUBSTITUTE(A1,CHAR(160),” “))), =SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A1,” “,”+“), CHAR(9),”+“), CHAR(10),”+“) replaces all white-space with a plus (helpful for manipulating addresses quickly), this following function filtered out the few roads that Google maps has images of the road for. =FILTER(G:G, (ISNUMBER(SEARCH(”W+MAIN+ST”, G:G))) + (ISNUMBER(SEARCH(“N+BROADWAY+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+PROSPECT+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LINN+AVE”,G:G))) + (ISNUMBER(SEARCH(“E+MAIN+ST”,G:G))) + (ISNUMBER(SEARCH(“N+LOCUST+AVE”,G:G))) + (ISNUMBER(SEARCH(“W+MILWAUKEE+ST”,G:G))) + (ISNUMBER(SEARCH(“N+PLEASANT+HILL+AVE”,G:G))) + (ISNUMBER(SEARCH(“S+LINN+AVE”,G:G))))\n As you can see above Google street view has very limited data in New Hampton which is why the URL’s we gathered for New Hampton had to be reduced. The interstate images are from 2018 while the images from the main road in the town is from 2009 and you can tell it gives you early 2000’s images vibes \nWe collected all the raw data last Friday right before the end of the day but cleaned the data this last Monday. The next few days we tried to scrape data from Zillow (no longer Trulia because we realized Zillow owns Trulia as of 2015?) but some interesting things we found is that you can look at houses not for sale and get Google images of those houses and estimates of the house worth. You can scrape data for houses that were recently sold and on sale currently for pictures unique from Google maps images. Finally we had a breakthrough on how to collect Zillow data using R and some elbow grease.\n\nThis code was initially just able to access Image links and addresses. It was then able to go to those image links, download the image, name it with the address, then export to a new folder. What other members of housing team did with this breakthrough after we hit a web scraping wall is investigate other items we could scrape such as amount of floors and cost which will be helpful for analyses of these cities.\nThe last accomplishment of this week was we now also have a program in R to compile image URL’s for the Google API using the addresses we found earlier this week. Example URL: https://maps.googleapis.com/maps/api/streetview?size=800x800&location=1009+REDBUD+DR,+SLATER+IOWA$key=(Google maps API key goes here)\n\nThis is a screenshot after I downloaded every Slater house we had a link to. We need to make a naming convention for the images we pull from Google and other sources to not only keep track of the images easily but also to make it easy for humans to decipher where an image is from (not just a number).\nFinally a little diagram was made to plan out what the algorithm will do that we are trying to build utilizing the AI models we plan to make (hopefully next week we can start cranking some out)"
  },
  {
    "objectID": "posts/Gavin-Fisher_week5/Week_Five.html",
    "href": "posts/Gavin-Fisher_week5/Week_Five.html",
    "title": "Week Five Blog",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. I was responsible for the new vegetation model and siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example, for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels. The new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\nWe also went to the ITAG conference. I went to Bullwall by Don McCraw, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule. I found the Bullwall talk very interesting which explained generally how cybersecurity works and how their product is an extra protection layer."
  },
  {
    "objectID": "posts/Gavin-Fisher_week6/Week_Six.html",
    "href": "posts/Gavin-Fisher_week6/Week_Six.html",
    "title": "Week Six Blog",
    "section": "",
    "text": "Monday DSPG assessed Grundy Center and New Hampton by taking pictures of houses and tracking characteristics of the houses. Tuesday we had a similar experience assessing Independence. A few details came to mind during these events, first in the Fulcrum app we had the option to evaluate roofs labeled AI Roof.\nAssuming that in future years roofing will be added to the list of characteristics evaluated I thought that when looking at roofs you need not only assess the quality of the roof but also the age. I believe there is a difference between a faded old roof and a new damaged roof. If DSPG goes to cities assessing the houses again it would be wise to sit down as a group and discuss what is considered good, fair, and poor in every single category. While assessing with other DSPG members I realized that people’s opinions of these categories greatly differed.\nWhile talking to residents of these towns I was made aware of other issues besides what we were evaluating such as plumbing and flooding issues. Although these are different than what the Housing team is currently directed towards maybe it could be projects for the future in DSPG.\nDue to the AI Roof category I left a space for the roofing model. If I have extra time I will fill this space with two models, one that evaluates whether the roof is damaged and one that evaluates the age. Additionally I added a spot for an AI window evaluator for a model that can predict whether there is a boarded up or broken window, inspired by Google images I downloaded from addresses considered poor or very poor in Des Moines. An option to look for in future years is an AI model that can look for multiple characteristics such as age and damage for roofs. Another we discussed this week is AI heatmaps which show where the AI is looking in the image to determine it’s guess. I really want to attempt this time permitting.\nNow for reading relief:\nI made this graph real quick just to visualize the counties which we are focusing on. Grundy Center is in Grundy County, New Hampton is in Chickasaw County, Independence is in Buchanan County, and Slater is in Story County.\n\nHere is a closer look at the fulcrum app we used to evaluate houses. The first image shows the different folders for each community, followed by the interface for a singular house evaluation.\n  \nNext for the code I built on the skeleton from last week mostly following the code plan diagram I made 2 weeks ago(?). As of right now house_evaluator.py can read in a folder with images in it (as if it was one house from multiple sources). The program will use the house_present_classifier, clear_image_classifier, and multiple_houses_classifier models. These models evaluate whether each image has a visible house, whether the house is obstructed, and if there are multiple houses visible. Ideally we want to evaluate an image with a house, minimal obstruction, and only one house visible. Each model will remove unwanted images from the list in the hopes of choosing only the best image of the house to evaluate.\nIf no image remains the program returns that a better image is needed of the house. If multiple good pictures of the house remain the program will randomly select one. I hope to implement looking at the date of the images before randomly selecting but we do not have dates of images stored from any source yet.\n\nNext the remaining image will be ran through the vegetation, siding, and gutter models. As I said last week the gutter model is on hold until I get more images of poor gutters and spaces in the code are there for roof and window models for either next year or my free time. All of the models predict pretty poorly still because we need a lot more data on bad house images but they are predicting at a 35-45% confidence rate currently.\nAfter I got all of the models in the program properly I started to work on writing to CSV files. The intention was to print each attribute such as vegetation in it’s respective column based on address. I used the CSV library which turned out to take longer than it seems giving me issues such as wiping the file clean, not writing any values, then writing values to the leftmost row available after address. I finally got the attributes to print to the correct column but printed on every row instead of only one row based on address. Sadat, our AI master student, used Pandas and could correctly do the correct task in about 10 lines where it took my method about 40 lines of code. Long story short I am now implementing pandas to manipulate the csv files.\nThis code shows the program checking if the columns exist in the csv and if not adding the column. Then the values of each is wrote to the CSV file. After discussion with Sadat I added the model confidence percentage so that as we upload more accurate models we can see how well they are preforming and also give an average confidence for a final report.\n\n\nIn summary this program so far can take in a folder of images, choose the best one, evaluate it’s vegetation, siding, and gutters, then print these attributes to a csv at the same address as the input folder. This python Script was nearing 400 lines of code due to tests and old code I commented out to save. To run many images through this program I think the easiest path would to make a parent folder full of folders labeled by address and inside of the address folders are the images for each address. I did not want a 400 line for loop to iterate through this parent folder so my goal for this weekend is to make two scripts. The first script will read in the files and iterate through them calling the second script each run to evaluate the images within and return house attributes. The first script will take these attributes and return them to a CSV file to the proper column and address. This will be essentially the same as what I have working now but it will be able to read and document many folders of houses rather than individual houses manually placed at a time."
  },
  {
    "objectID": "posts/House_blog/House_blog.html",
    "href": "posts/House_blog/House_blog.html",
    "title": "Housing Team Recap Week Two",
    "section": "",
    "text": "Project is more planned out\nPersonal and team Blogs are mostly created\nHalf of backup data is uploaded to GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#happies",
    "href": "posts/House_blog/House_blog.html#happies",
    "title": "Housing Team Recap Week Two",
    "section": "Happies",
    "text": "Happies\nWe are in the lead for team datacamp points at about 90k total\nBlogs:\nAngelina\nGavin\nKailyn"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#crappies",
    "href": "posts/House_blog/House_blog.html#crappies",
    "title": "Housing Team Recap Week Two",
    "section": "Crappies",
    "text": "Crappies\nThe Blogs are difficult to work with and get set up"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "href": "posts/House_blog/House_blog.html#cool-technical-things-we-learned-this-week",
    "title": "Housing Team Recap Week Two",
    "section": "Cool Technical Things We Learned This Week",
    "text": "Cool Technical Things We Learned This Week\nHow to created webpages in R studio\nMore basics in R such as making matrices\n\n\n\n\n\nMatrices\n\n\nHow to use Tidycensus for importing variable codes\n\n\n\n\n\nVariable Loading\n\n\nPractice with GitHub"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "href": "posts/House_blog/House_blog.html#tidycensus-graphs",
    "title": "Housing Team Recap Week Two",
    "section": "Tidycensus Graphs",
    "text": "Tidycensus Graphs\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nAngelina\n\n\n\n\n\n\n\nKailyn\n\n\n\n\n\n\n\nGavin"
  },
  {
    "objectID": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "href": "posts/House_blog/House_blog.html#random-facts-for-chris",
    "title": "Housing Team Recap Week Two",
    "section": "Random Facts for Chris",
    "text": "Random Facts for Chris\nFrom statistic brain research institute, “…in an average hour, there are over 61,000 Americans airborne over the United States.”\nEvery second, 75 McDonalds burgers are eaten\nIn 1890, the Hollerith Machine was used to tabulate Census data. Technically, this could be called the first computer device."
  },
  {
    "objectID": "posts/House_blog/House_blog.html#questions-discussion",
    "href": "posts/House_blog/House_blog.html#questions-discussion",
    "title": "Housing Team Recap Week Two",
    "section": "Questions/ Discussion",
    "text": "Questions/ Discussion\nUnrelated from work - what do people do here during the summer?"
  },
  {
    "objectID": "posts/Housing-Week-Five/House_Week_Five.html",
    "href": "posts/Housing-Week-Five/House_Week_Five.html",
    "title": "Housing Team Recap Week Five",
    "section": "",
    "text": "First a model was made following the graphic from last week, the only added characteristics include a variable to hold the best image once it is selected, a variable to track if an image was randomly chosen by the algorithm, and that the program exits if no good picture is available. Otherwise the algorithm’s skeleton is made as planned reading in images from a folder, a spot to add models that evaluate image quality, spots to evaluate characteristics, then finally a section to write to a csv file. Minimal code was added to the skeleton at the beginning of the week.\n\n\n\nAfter the initial Skeleton code was made the first vegetation model was used to demonstrate how we would read in the models that we create then push images through the model to get a prediction of the output.\n\nNext using the original vegetation model as an example our team was able to create five additional models in Google Colab, house present model, clear image model, multiple houses model, vegetation model (new), and the siding model. The gutter model will be added later once images of damaged gutters are obtained. Images downloaded from the Google API had to be sorted into 2-3 categories for each model in order to train. Sorting is definitely in the crappies category due to it taking hours with a lack of poor images resulting. We will need to find a more efficient way to sort images for model training than moving images into folders.\n\nMost of the models were very inaccurate due to the lack of data on poor house images. For example for the siding model there were plenty of images of good siding, less images with chipped paint, and very few images with broken siding panels.\n\nThe new model also shows the sample images in a more comprehensible way by showing the label names rather than just a binary label.\n\nFinally after making the initial models they were added to the house_evaluator python script. Currently, the program prints the values returned by the models to a test CSV. Once more training images are obtained to improve the models accuracy the python script will print the results to the address of the images provided in the houses_databases CSV files. This image shows how the models with more than one label can return multiple options.\n\n\n\n\nOn top of the AI models we needed to start filling in other characteristics about the addresses which we have collected. Although there have been a few errors in duplicate images and incorrect addresses we were able to link what pictures we currently have from Google into CSV files for each city. We can continue to grab data from Zillow.com and start collecting on Realtor.com\n\n\n\n\nIn order to get a head start on spatial mapping which we will use as part of our end results demonstration, we took a look into Geospatial Mapping on Datacamp, got back into using the census data, and took a look at Kyle Walkers TidyCensus book (online free!).\nUsing the US census data we started to look at changes in Iowa population from 2000-2020. \n\nWe tested out QGIS by mapping Slater and New Hampton using the lat long information off of the Google API.\n\n\n\n\n\n\nWe were able to meet with many vendors to learn about their companies and introduce our program and project. There were also many talks about Cyber Security, GIS, and IT.\nPresentations we attended: Modernize Your ArcGIS Web AppBuilder Apps Using Experience Builder by Mitch Winiecki, Bullwall by Don McCraw, ESRI Hands on Learning Lab by Rick Zellmer, Modernizing Utility Operations with ArcGIS by Chase Fisher, Ransomware & Other Cyber Risks: Making Sense of Cybersecurity Headlines and Actionable Items for Mitigation by Ben, and finally The Story of Water at the Missouri DOC and the new Hydrography and Wetland Modelling by Mike Tully. Link to full schedule"
  }
]